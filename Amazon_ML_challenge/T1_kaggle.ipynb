{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ===================== Imports & Global Setup =====================\n",
    "# pip: transformers==4.41.1\n",
    "import os\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter  # optional, used only on rank 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import torch.backends.cuda as cuda_backends\n",
    "cuda_backends.matmul.allow_tf32 = True\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.set_verbosity_info()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# %% ===================== Units & canonicalization =====================\n",
    "UNIT_SCALES: Dict[str, Tuple[str, float]] = {\n",
    "    'lb': ('g', 453.59237), 'lbs': ('g', 453.59237),\n",
    "    'pound': ('g', 453.59237), 'pounds': ('g', 453.59237),\n",
    "    'kg': ('g', 1000.0), 'kilogram': ('g', 1000.0), 'kilograms': ('g', 1000.0),\n",
    "    'g': ('g', 1.0), 'gram': ('g', 1.0), 'grams': ('g', 1.0),\n",
    "    'mg': ('g', 1e-3), 'milligram': ('g', 1e-3), 'milligrams': ('g', 1e-3),\n",
    "    'oz': ('g', 28.349523125), 'ounce': ('g', 28.349523125), 'ounces': ('g', 28.349523125),\n",
    "\n",
    "    'l': ('ml', 1000.0), 'liter': ('ml', 1000.0), 'litre': ('ml', 1000.0),\n",
    "    'liters': ('ml', 1000.0), 'litres': ('ml', 1000.0),\n",
    "    'ml': ('ml', 1.0), 'milliliter': ('ml', 1.0), 'milliliters': ('ml', 1.0),\n",
    "    'millilitre': ('ml', 1.0), 'millilitres': ('ml', 1.0),\n",
    "    'fl oz': ('ml', 29.5735295625), 'floz': ('ml', 29.5735295625),\n",
    "\n",
    "    'in': ('cm', 2.54), 'inch': ('cm', 2.54), 'inches': ('cm', 2.54),\n",
    "    'ft': ('cm', 30.48), 'foot': ('cm', 30.48), 'feet': ('cm', 30.48),\n",
    "    'cm': ('cm', 1.0), 'mm': ('cm', 0.1),\n",
    "\n",
    "    'count': ('count', 1.0), 'ct': ('count', 1.0),\n",
    "    'pcs': ('count', 1.0), 'piece': ('count', 1.0), 'pieces': ('count', 1.0),\n",
    "}\n",
    "\n",
    "def _norm_unit(u: str) -> str:\n",
    "    u = u.lower().strip()\n",
    "    u = re.sub(r'[.\\s]+', ' ', u)\n",
    "    u = u.replace('fluid ounce', 'fl oz').replace('fluid ounces', 'fl oz')\n",
    "    u = u.replace('fl. oz', 'fl oz').replace('fl-oz', 'fl oz')\n",
    "    u = u.replace('ounces', 'oz').replace('ounce', 'oz')\n",
    "    u = u.replace('ct.', 'ct')\n",
    "    return u\n",
    "\n",
    "def canonicalize_unit(raw_unit: str) -> Tuple[str, float]:\n",
    "    if not isinstance(raw_unit, str) or not raw_unit.strip():\n",
    "        return '<unk>', 1.0\n",
    "    u = _norm_unit(raw_unit)\n",
    "    if u in UNIT_SCALES:\n",
    "        return UNIT_SCALES[u]\n",
    "    u2 = u.replace(' ', '')\n",
    "    if u2 in UNIT_SCALES:\n",
    "        return UNIT_SCALES[u2]\n",
    "    if u.endswith('s') and u[:-1] in UNIT_SCALES:\n",
    "        return UNIT_SCALES[u[:-1]]\n",
    "    return '<unk>', 1.0\n",
    "\n",
    "\n",
    "# %% ===================== Patterns & feature extraction =====================\n",
    "VALUE_RE = re.compile(r'Value:\\s*([+-]?\\d+(?:\\.\\d+)?)', re.IGNORECASE)\n",
    "UNIT_RE  = re.compile(r'Unit:\\s*([A-Za-z.\\-\\s]+)', re.IGNORECASE)\n",
    "\n",
    "PACK_PATTERNS = [\n",
    "    r'[Pp]ack\\s*of\\s*(\\d+)',\n",
    "    r'(\\d+)\\s*[Pp]ack\\b',\n",
    "    r'(\\d+)\\s*-\\s*[Cc]ount',\n",
    "    r'(\\d+)\\s*(?:ct|count)\\b',\n",
    "    r'[x×]\\s*(\\d+)\\b',\n",
    "    r'\\b(\\d+)\\s*[x×]\\s*\\d+\\s*(?:oz|fl\\s*oz|g|ml)\\b',\n",
    "    r'\\(.*?[Pp]ack\\s*of\\s*(\\d+).*?\\)',\n",
    "    r'\\bcase\\s*of\\s*(\\d+)\\b',\n",
    "]\n",
    "PACK_RE = re.compile('|'.join(PACK_PATTERNS))\n",
    "\n",
    "PREMIUM_KWS = ['organic', 'premium', 'gourmet', 'artisan', 'natural', 'handcrafted', 'imported', 'luxury']\n",
    "BULK_KWS    = ['pack', 'case', 'bulk', 'bundle', 'family size', 'wholesale']\n",
    "\n",
    "CAT_RULES = [\n",
    "    ('soup',     r'\\bsoup\\b|ramen|broth'),\n",
    "    ('sauce',    r'\\bsauce\\b|ketchup|mustard|mayo|dressing|marinara|salsa'),\n",
    "    ('cookies',  r'\\bcookies?\\b|biscuit'),\n",
    "    ('candy',    r'candy|chocolate|gummy|toffee|mint'),\n",
    "    ('snack',    r'\\bchips?\\b|pretzel|popcorn|cracker|snack'),\n",
    "    ('spice',    r'\\bspice\\b|seasoning|masala|herb|salt|pepper'),\n",
    "    ('beverage', r'coffee|tea|soda|drink|beverage|juice'),\n",
    "    ('grains',   r'rice|pasta|noodle|flour|oats|cereal'),\n",
    "    ('oil',      r'\\boil\\b|olive oil|canola|sunflower'),\n",
    "    ('gift',     r'gift|basket|hamper'),\n",
    "    ('dairy',    r'cheese|milk|butter|yogurt'),\n",
    "]\n",
    "\n",
    "PROTEIN_RE  = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(?:g|grams?)\\s*(?:of\\s*)?protein', re.IGNORECASE)\n",
    "FIBER_RE    = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(?:g|grams?)\\s*(?:of\\s*)?fiber', re.IGNORECASE)\n",
    "CALORIES_RE = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*calories?', re.IGNORECASE)\n",
    "SUGAR_RE    = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(?:g|grams?)\\s*(?:of\\s*)?sugar', re.IGNORECASE)\n",
    "ITEM_NAME_RE = re.compile(r'Item Name:\\s*(.+?)(?=\\n|Bullet Point|Product Description|$)', re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'Value:.*?Unit:.*?(?:\\n|$)', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_value_unit(text: str) -> Tuple[float, str]:\n",
    "    if not isinstance(text, str):\n",
    "        return 0.0, 'Unknown'\n",
    "    vm = VALUE_RE.search(text)\n",
    "    um = UNIT_RE.search(text)\n",
    "    value = float(vm.group(1)) if vm else 0.0\n",
    "    unit  = um.group(1).strip() if um else 'Unknown'\n",
    "    return value, unit\n",
    "\n",
    "def detect_pack_size(text: str) -> int:\n",
    "    if not isinstance(text, str):\n",
    "        return 1\n",
    "    m = PACK_RE.search(text)\n",
    "    if not m: return 1\n",
    "    for g in m.groups():\n",
    "        if g is not None:\n",
    "            try:\n",
    "                return max(1, int(g))\n",
    "            except:\n",
    "                continue\n",
    "    return 1\n",
    "\n",
    "def extract_brand(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"<unk>\"\n",
    "    m = re.search(r'Item Name:\\s*(.+)', text, flags=re.IGNORECASE)\n",
    "    s = m.group(1) if m else text\n",
    "    s = s.split('|')[0].split(' - ')[0].split(',')[0].strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    if len(s) == 0:\n",
    "        return \"<unk>\"\n",
    "    first_tok = s.split(' ')[0]\n",
    "    if len(first_tok) <= 2:\n",
    "        return s[:40].strip() or \"<unk>\"\n",
    "    return first_tok[:40]\n",
    "\n",
    "def category_bucket(text: str) -> str:\n",
    "    low = text.lower() if isinstance(text, str) else \"\"\n",
    "    for cat, pattern in CAT_RULES:\n",
    "        if re.search(pattern, low):\n",
    "            return cat\n",
    "    return 'other'\n",
    "\n",
    "def extract_item_name(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    m = ITEM_NAME_RE.search(text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_sub_category(item_name: str) -> str:\n",
    "    if not item_name: return \"Other\"\n",
    "    return item_name[:50] or \"Other\"\n",
    "\n",
    "def extract_flavor_profile(item_name: str, text: str) -> str:\n",
    "    flavors = re.search(r'(mild|original|creamy|blue cheese|sherry|basil|key lime|black raspberry|strawberry banana|cookie dough|other flavors)', text.lower())\n",
    "    return flavors.group(1) if flavors else \"Other\"\n",
    "\n",
    "def extract_features_row(text: str) -> Dict[str, Any]:\n",
    "    if not isinstance(text, str): text = \"\"\n",
    "    value_raw, unit_raw = extract_value_unit(text)\n",
    "    canon_unit, scale = canonicalize_unit(unit_raw)\n",
    "    value_canon = value_raw * scale\n",
    "\n",
    "    pack_size = detect_pack_size(text)\n",
    "    value_per_pack = value_canon / max(pack_size, 1)\n",
    "    low = text.lower()\n",
    "\n",
    "    total_ml = value_canon if canon_unit == 'ml' else 0.0\n",
    "    total_g  = value_canon if canon_unit == 'g'  else 0.0\n",
    "    is_bulk = 1.0 if (pack_size >= 6 or total_ml >= 1500 or total_g >= 1000) else 0.0\n",
    "\n",
    "    item_name = extract_item_name(text)\n",
    "    sub_category = extract_sub_category(item_name)\n",
    "    flavor_profile = extract_flavor_profile(item_name, text)\n",
    "\n",
    "    protein_m = PROTEIN_RE.search(text)\n",
    "    fiber_m = FIBER_RE.search(text)\n",
    "    calories_m = CALORIES_RE.search(text)\n",
    "    sugar_m = SUGAR_RE.search(text)\n",
    "    protein_grams = float(protein_m.group(1)) if protein_m else 0.0\n",
    "    fiber_grams = float(fiber_m.group(1)) if fiber_m else 0.0\n",
    "    calories_per_serving = float(calories_m.group(1)) if calories_m else 0.0\n",
    "    sugar_grams = float(sugar_m.group(1)) if sugar_m else 0.0\n",
    "\n",
    "    is_high_protein = 1.0 if 'high protein' in low or protein_grams > 10 else 0.0\n",
    "    is_low_calorie = 1.0 if 'low calorie' in low or (calories_per_serving > 0 and calories_per_serving < 50) else 0.0\n",
    "\n",
    "    # Log/ratio/inverse features\n",
    "    log_value_canon = np.log1p(value_canon)\n",
    "    log_pack_size = np.log1p(pack_size)\n",
    "    log_value_per_pack = np.log1p(value_per_pack)\n",
    "    log_total_g = np.log1p(total_g)\n",
    "    log_total_ml = np.log1p(total_ml)\n",
    "    value_to_pack_ratio = value_canon / max(pack_size, 1)\n",
    "\n",
    "    is_weight_unit = 1.0 if canon_unit == 'g' else 0.0\n",
    "    is_volume_unit = 1.0 if canon_unit == 'ml' else 0.0\n",
    "    is_count_unit = 1.0 if canon_unit == 'count' else 0.0\n",
    "\n",
    "    inv_total_g = 1.0 / total_g if total_g > 0 else 0.0\n",
    "    inv_total_ml = 1.0 / total_ml if total_ml > 0 else 0.0\n",
    "    inv_value_canon = 1.0 / value_canon if value_canon > 0 else 0.0\n",
    "    inv_pack_size = 1.0 / pack_size\n",
    "\n",
    "    return {\n",
    "        'value': float(value_raw),\n",
    "        'unit': unit_raw,\n",
    "        'value_canon': float(value_canon),\n",
    "        'canon_unit': canon_unit,\n",
    "        'pack_size': float(pack_size),\n",
    "        'value_per_pack': float(value_per_pack),\n",
    "        'total_ml': float(total_ml),\n",
    "        'total_g': float(total_g),\n",
    "\n",
    "        'log_value_canon': float(log_value_canon),\n",
    "        'log_pack_size': float(log_pack_size),\n",
    "        'log_value_per_pack': float(log_value_per_pack),\n",
    "        'log_total_g': float(log_total_g),\n",
    "        'log_total_ml': float(log_total_ml),\n",
    "        'value_to_pack_ratio': float(value_to_pack_ratio),\n",
    "        'is_weight_unit': float(is_weight_unit),\n",
    "        'is_volume_unit': float(is_volume_unit),\n",
    "        'is_count_unit': float(is_count_unit),\n",
    "        'inv_total_g': float(inv_total_g),\n",
    "        'inv_total_ml': float(inv_total_ml),\n",
    "        'inv_value_canon': float(inv_value_canon),\n",
    "        'inv_pack_size': float(inv_pack_size),\n",
    "\n",
    "        'text_length': len(text),\n",
    "        'word_count': len(text.split()),\n",
    "        'bullet_count': float(text.count('Bullet Point')),\n",
    "        'premium_count': float(sum(kw in low for kw in PREMIUM_KWS)),\n",
    "        'bulk_count': float(sum(kw in low for kw in BULK_KWS)),\n",
    "        'has_number': float(1 if re.search(r'\\d', text) else 0),\n",
    "        'is_bulk': is_bulk,\n",
    "        'brand': extract_brand(text),\n",
    "        'category_bucket': category_bucket(text),\n",
    "        'sub_category': sub_category,\n",
    "        'flavor_profile': flavor_profile,\n",
    "\n",
    "        'is_organic': 1.0 if 'organic' in low else 0.0,\n",
    "        'is_gluten_free': 1.0 if 'gluten-free' in low or 'gluten free' in low else 0.0,\n",
    "        'is_vegan': 1.0 if 'vegan' in low or 'plant-based' in low else 0.0,\n",
    "        'is_keto_friendly': 1.0 if 'keto' in low or 'low carb' in low else 0.0,\n",
    "        'is_non_gmo': 1.0 if 'non-gmo' in low or 'gmo free' in low else 0.0,\n",
    "        'is_kosher': 1.0 if 'kosher' in low else 0.0,\n",
    "        'is_dairy_free': 1.0 if 'dairy free' in low else 0.0,\n",
    "        'is_nut_free': 1.0 if 'nut-free' in low else 0.0,\n",
    "        'is_soy_free': 1.0 if 'soy free' in low else 0.0,\n",
    "        'is_sugar_free': 1.0 if 'sugar free' in low or 'no sugar added' in low else 0.0,\n",
    "        'is_low_fat': 1.0 if 'low fat' in low or '0 grams trans fat' in low else 0.0,\n",
    "        'is_low_sodium': 1.0 if 'low sodium' in low or 'no salt added' in low else 0.0,\n",
    "        'is_all_natural': 1.0 if 'all natural' in low or 'natural' in low else 0.0,\n",
    "        'is_fair_trade': 1.0 if 'fair trade' in low else 0.0,\n",
    "        'is_usda_certified': 1.0 if 'usda organic' in low else 0.0,\n",
    "\n",
    "        'protein_grams': protein_grams,\n",
    "        'fiber_grams': fiber_grams,\n",
    "        'calories_per_serving': calories_per_serving,\n",
    "        'sugar_grams': sugar_grams,\n",
    "        'has_vitamins': 1.0 if 'vitamin' in low or 'vitamins' in low else 0.0,\n",
    "        'has_minerals': 1.0 if any(k in low for k in ['potassium', 'iron', 'magnesium', 'zinc']) else 0.0,\n",
    "        'has_antioxidants': 1.0 if 'antioxidants' in low else 0.0,\n",
    "        'is_high_protein': is_high_protein,\n",
    "        'is_low_calorie': is_low_calorie,\n",
    "\n",
    "        'is_ready_to_eat': 1.0 if 'ready to eat' in low else 0.0,\n",
    "        'is_easy_to_prepare': 1.0 if 'easy to prepare' in low or 'instant' in low else 0.0,\n",
    "        'is_versatile': 1.0 if 'versatile' in low or 'multiple uses' in low else 0.0,\n",
    "        'is_snack': 1.0 if 'snack' in low or 'on-the-go' in low else 0.0,\n",
    "        'is_beverage': 1.0 if any(k in low for k in ['drink', 'beverage', 'tea', 'coffee', 'juice']) else 0.0,\n",
    "        'is_baking_ingredient': 1.0 if any(k in low for k in ['baking', 'flour', 'powder']) else 0.0,\n",
    "        'has_no_preservatives': 1.0 if 'no preservatives' in low else 0.0,\n",
    "        'is_shelf_stable': 1.0 if any(k in low for k in ['shelf stable', 'canned']) else 0.0,\n",
    "\n",
    "        'has_ingredients_list': 1.0 if 'ingredients:' in low else 0.0,\n",
    "        'has_product_description': 1.0 if 'product description:' in low else 0.0,\n",
    "    }\n",
    "\n",
    "def build_numeric_table(catalog_series: pd.Series) -> pd.DataFrame:\n",
    "    feats = [extract_features_row(t) for t in catalog_series]\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "\n",
    "# %% ===================== Metrics & Loss =====================\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    y_true_arr = np.asarray(y_true, dtype=float)\n",
    "    y_pred_arr = np.asarray(y_pred, dtype=float)\n",
    "    y_true_arr = np.abs(y_true_arr)\n",
    "    y_pred_arr = np.abs(y_pred_arr)\n",
    "    denom = (y_true_arr + y_pred_arr) / 2.0\n",
    "    return float(np.mean(np.abs(y_pred_arr - y_true_arr) / (denom + 1e-8)) * 100.0)\n",
    "\n",
    "def smape_np(pred, true):\n",
    "    pred = np.abs(pred)\n",
    "    true = np.abs(true)\n",
    "    den = (pred + true) / 2.0\n",
    "    return float(np.mean(np.abs(pred - true) / (den + 1e-8)) * 100.0)\n",
    "\n",
    "def smape_loss(pred, true):\n",
    "    pred_abs = torch.abs(pred)\n",
    "    true_abs = torch.abs(true)\n",
    "    denom = (pred_abs + true_abs) / 2.0\n",
    "    return torch.mean(torch.abs(pred - true) / (denom + 1e-8)) * 100.0\n",
    "\n",
    "def combined_loss(pred_log, true_log, pred_raw, true_raw, alpha=0.7):\n",
    "    l1_loss = torch.nn.functional.l1_loss(pred_log, true_log)\n",
    "    smape_val = smape_loss(pred_raw, true_raw)\n",
    "    return alpha * l1_loss + (1.0 - alpha) * smape_val / 100.0\n",
    "\n",
    "\n",
    "# %% ===================== Dataset =====================\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, unit_encoder=None, cat_encoder=None, flav_encoder=None, scaler=None,\n",
    "                 max_length=384, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "\n",
    "        feats = [extract_features_row(t) for t in self.df['catalog_content'].fillna(\"\")]\n",
    "        self.feat_df = pd.DataFrame(feats)\n",
    "\n",
    "        self.numeric_cols = [col for col in self.feat_df.columns \n",
    "                             if pd.api.types.is_numeric_dtype(self.feat_df[col])]\n",
    "        X_num = self.feat_df[self.numeric_cols].fillna(0).values.astype(np.float32)\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_num = self.scaler.fit_transform(X_num)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            X_num = self.scaler.transform(X_num)\n",
    "        self.X_num = X_num.astype(np.float32)\n",
    "\n",
    "        if unit_encoder is None:\n",
    "            self.unit_le = LabelEncoder()\n",
    "            units = sorted(set(self.feat_df['canon_unit'].fillna('<unk>').tolist()) | {'<unk>'})\n",
    "            self.unit_le.fit(units)\n",
    "        else:\n",
    "            self.unit_le = unit_encoder\n",
    "\n",
    "        if cat_encoder is None:\n",
    "            self.cat_le = LabelEncoder()\n",
    "            cats = sorted(set(self.feat_df['category_bucket'].fillna('other').tolist()) | {'other'})\n",
    "            self.cat_le.fit(cats)\n",
    "        else:\n",
    "            self.cat_le = cat_encoder\n",
    "\n",
    "        if flav_encoder is None:\n",
    "            self.flav_le = LabelEncoder()\n",
    "            flavs = sorted(set(self.feat_df['flavor_profile'].fillna('Other').tolist()) | {'Other'})\n",
    "            self.flav_le.fit(flavs)\n",
    "        else:\n",
    "            self.flav_le = flav_encoder\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        txt = clean_text(row['catalog_content'])\n",
    "        enc = self.tok(txt, max_length=self.max_length, padding='max_length',\n",
    "                       truncation=True, return_tensors='pt')\n",
    "        u = self.feat_df.iloc[i]['canon_unit'] or '<unk>'\n",
    "        try:\n",
    "            u_id = int(self.unit_le.transform([u])[0])\n",
    "        except Exception:\n",
    "            u_id = int(self.unit_le.transform(['<unk>'])[0])\n",
    "\n",
    "        c = self.feat_df.iloc[i]['category_bucket'] or 'other'\n",
    "        c_id = int(self.cat_le.transform([c])[0])\n",
    "\n",
    "        f = self.feat_df.iloc[i]['flavor_profile'] or 'Other'\n",
    "        f_id = int(self.flav_le.transform([f])[0])\n",
    "\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'num': torch.tensor(self.X_num[i], dtype=torch.float32),\n",
    "            'unit': torch.tensor(u_id, dtype=torch.long),\n",
    "            'cat': torch.tensor(c_id, dtype=torch.long),\n",
    "            'flav': torch.tensor(f_id, dtype=torch.long),\n",
    "            'sample_id': row['sample_id'],\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            item['ylog'] = torch.tensor(np.log1p(float(row['price'])), dtype=torch.float32)\n",
    "            item['y_raw'] = torch.tensor(float(row['price']), dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "\n",
    "# %% ===================== Model =====================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, name, num_num, num_units, num_cats, num_flavs, hidden=768, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.enc = AutoModel.from_pretrained(name, trust_remote_code=False)\n",
    "        d = self.enc.config.hidden_size\n",
    "        self.unit_emb = nn.Embedding(num_units, 16)\n",
    "        self.cat_emb = nn.Embedding(num_cats, 16)\n",
    "        self.flav_emb = nn.Embedding(num_flavs, 16)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d + num_num + 16 + 16 + 16, hidden),\n",
    "            nn.LayerNorm(hidden), \n",
    "            nn.SiLU(), \n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.LayerNorm(hidden//2), \n",
    "            nn.SiLU(), \n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden//2, hidden//4),\n",
    "            nn.LayerNorm(hidden//4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(drop/2),\n",
    "            nn.Linear(hidden//4, 1)\n",
    "        )\n",
    "\n",
    "    def mean_pool(self, last_hidden, mask):\n",
    "        mask = mask.unsqueeze(-1).float()\n",
    "        return (last_hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
    "\n",
    "    def forward(self, ids, mask, num, unit, cat, flav):\n",
    "        out = self.enc(input_ids=ids, attention_mask=mask).last_hidden_state\n",
    "        txt = self.mean_pool(out, mask)\n",
    "        u = self.unit_emb(unit)\n",
    "        c = self.cat_emb(cat)\n",
    "        f = self.flav_emb(flav)\n",
    "        x = torch.cat([txt, num, u, c, f], 1)\n",
    "        return self.mlp(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# %% ===================== Utils: DDP helpers =====================\n",
    "def set_seed(seed: int, rank: int = 0):\n",
    "    s = seed + rank\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    return dist.is_available() and dist.is_initialized()\n",
    "\n",
    "def get_rank():\n",
    "    return dist.get_rank() if is_dist_avail_and_initialized() else 0\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "def all_gather_numpy(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gather variable-length numpy arrays across ranks to rank 0.\"\"\"\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return arr\n",
    "    gathered = [None for _ in range(dist.get_world_size())]\n",
    "    dist.all_gather_object(gathered, arr)\n",
    "    if is_main_process():\n",
    "        return np.concatenate([g for g in gathered if g is not None]) if len(gathered) else arr\n",
    "    return arr  # non-main returns its local (unused by caller)\n",
    "\n",
    "\n",
    "# %% ===================== Main (DDP) =====================\n",
    "def main():\n",
    "    # -------- Config --------\n",
    "    SEED = 42\n",
    "    DATA = \"dataset\"\n",
    "    OUT = \"/models/T1\"\n",
    "    os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "    MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    BATCH = 24                 # per-GPU\n",
    "    EPOCHS = 15\n",
    "    WARMUP_E = 2\n",
    "    MAXL = 384\n",
    "    LR_ENC = 1.5e-5\n",
    "    LR_HEAD = 1.2e-3\n",
    "    ALPHA = 0.7\n",
    "\n",
    "    # -------- DDP init (or single process fallback) --------\n",
    "    ddp = False\n",
    "    local_rank_env = os.environ.get(\"LOCAL_RANK\", None)\n",
    "    if local_rank_env is not None:\n",
    "        ddp = True\n",
    "        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "        local_rank = int(local_rank_env)\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        dev = torch.device(f\"cuda:{local_rank}\")\n",
    "    else:\n",
    "        local_rank = 0\n",
    "        dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    set_seed(SEED, rank=local_rank)\n",
    "    main_proc = is_main_process()\n",
    "\n",
    "    if main_proc:\n",
    "        print(f\"[Boot] transformer_tabular_v2 (DDP={ddp}) starting...\", flush=True)\n",
    "        print(f\"[Setup] Device: {dev} (rank={get_rank()})\", flush=True)\n",
    "        print(f\"[Phase 1] Model: {MODEL_NAME}, MaxLen: {MAXL}, Batch/GPU: {BATCH}\", flush=True)\n",
    "\n",
    "    # -------- Data --------\n",
    "    train_csv = os.path.join(DATA, \"train.csv\")\n",
    "    if not os.path.exists(train_csv):\n",
    "        if main_proc:\n",
    "            print(f\"[Error] Missing dataset file: {train_csv}\", flush=True)\n",
    "        if ddp:\n",
    "            dist.barrier()\n",
    "            dist.destroy_process_group()\n",
    "        return\n",
    "\n",
    "    if main_proc:\n",
    "        print(\"[Load] Reading train.csv ...\", flush=True)\n",
    "    df = pd.read_csv(train_csv)\n",
    "    if main_proc:\n",
    "        print(f\"[Load] Rows: {len(df)}, Columns: {list(df.columns)}\", flush=True)\n",
    "        print(\"[Split] Train/Val split...\", flush=True)\n",
    "    tr, va = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "    if main_proc:\n",
    "        print(f\"[Split] Train: {len(tr)}, Val: {len(va)}\", flush=True)\n",
    "\n",
    "    # -------- Tokenizer --------\n",
    "    if main_proc:\n",
    "        print(f\"[HF] Loading tokenizer/model: {MODEL_NAME}\", flush=True)\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=False)\n",
    "\n",
    "    # -------- Build initial datasets to infer encoders/scaler --------\n",
    "    dtr_tmp = PriceDataset(tr, tok, max_length=MAXL, is_test=False)\n",
    "    dva_tmp = PriceDataset(va, tok, max_length=MAXL, is_test=False,\n",
    "                           unit_encoder=dtr_tmp.unit_le, cat_encoder=dtr_tmp.cat_le,\n",
    "                           flav_encoder=dtr_tmp.flav_le, scaler=dtr_tmp.scaler)\n",
    "\n",
    "    num_num = len(dtr_tmp.numeric_cols)\n",
    "    if main_proc:\n",
    "        print(f\"[Model] Building with {num_num} numeric features\", flush=True)\n",
    "\n",
    "    # -------- Model --------\n",
    "    model = Model(MODEL_NAME, num_num=num_num,\n",
    "                  num_units=len(dtr_tmp.unit_le.classes_),\n",
    "                  num_cats=len(dtr_tmp.cat_le.classes_),\n",
    "                  num_flavs=len(dtr_tmp.flav_le.classes_),\n",
    "                  hidden=768)\n",
    "    model.to(dev)\n",
    "\n",
    "    # memory: enable checkpointing on encoder and disable cache\n",
    "# MPNetModel does not support gradient checkpointing — don't call it.\n",
    "    if hasattr(model, \"enc\") and hasattr(model.enc, \"config\"):\n",
    "        model.enc.config.use_cache = False  # still helpful for memory\n",
    "\n",
    "\n",
    "    # -------- Rebuild datasets on every rank (keep same encoders/scaler) --------\n",
    "    dtr = PriceDataset(tr, tok, max_length=MAXL, is_test=False,\n",
    "                       unit_encoder=dtr_tmp.unit_le, cat_encoder=dtr_tmp.cat_le,\n",
    "                       flav_encoder=dtr_tmp.flav_le, scaler=dtr_tmp.scaler)\n",
    "    dva = PriceDataset(va, tok, max_length=MAXL, is_test=False,\n",
    "                       unit_encoder=dtr_tmp.unit_le, cat_encoder=dtr_tmp.cat_le,\n",
    "                       flav_encoder=dtr_tmp.flav_le, scaler=dtr_tmp.scaler)\n",
    "\n",
    "    # -------- Samplers & Loaders --------\n",
    "    if ddp:\n",
    "        sampler_tr = DistributedSampler(dtr, shuffle=True, drop_last=False)\n",
    "        sampler_va = DistributedSampler(dva, shuffle=False, drop_last=False)\n",
    "    else:\n",
    "        sampler_tr = None\n",
    "        sampler_va = None\n",
    "\n",
    "    ltr = DataLoader(dtr, batch_size=BATCH, shuffle=(sampler_tr is None),\n",
    "                     sampler=sampler_tr, num_workers=2, pin_memory=(dev.type=='cuda'))\n",
    "    lva = DataLoader(dva, batch_size=BATCH, shuffle=False,\n",
    "                     sampler=sampler_va, num_workers=2, pin_memory=(dev.type=='cuda'))\n",
    "\n",
    "    # -------- Optimizer & Scheduler --------\n",
    "    enc_params = list(model.enc.parameters())\n",
    "    head_params = [p for n, p in model.named_parameters() if not n.startswith('enc')]\n",
    "\n",
    "    opt = torch.optim.AdamW([\n",
    "        {'params': enc_params, 'lr': LR_ENC, 'weight_decay': 0.01},\n",
    "        {'params': head_params, 'lr': LR_HEAD, 'weight_decay': 0.0},\n",
    "    ])\n",
    "\n",
    "    total_steps = max(1, len(ltr)) * EPOCHS\n",
    "    sched = get_cosine_schedule_with_warmup(opt, num_warmup_steps=max(1, int(0.05 * total_steps)),\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(dev.type == 'cuda'))\n",
    "\n",
    "    # -------- Warmup: freeze encoder initially --------\n",
    "    for p in model.enc.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # -------- Wrap with DDP --------\n",
    "    if ddp:\n",
    "        model = DDP(model, device_ids=[dev.index], output_device=dev.index, find_unused_parameters=False)\n",
    "\n",
    "    # -------- Train loop --------\n",
    "    best = 1e9\n",
    "    patience = 6\n",
    "    bad = 0\n",
    "\n",
    "    if main_proc:\n",
    "        print(\"[Train] Starting epochs...\", flush=True)\n",
    "        print(f\"[Loss] Using combined loss: {ALPHA}*L1 + {1-ALPHA}*SMAPE\", flush=True)\n",
    "\n",
    "    try:\n",
    "        for ep in range(EPOCHS):\n",
    "            if ddp:\n",
    "                sampler_tr.set_epoch(ep)\n",
    "\n",
    "            if main_proc:\n",
    "                print(f\"\\n[Epoch {ep+1}/{EPOCHS}] ----------------------------\", flush=True)\n",
    "                tbar = tqdm(total=len(ltr), desc=\"train\", mininterval=1.0, leave=False)\n",
    "            else:\n",
    "                tbar = None\n",
    "\n",
    "            # Unfreeze after warmup epochs\n",
    "            if ep == WARMUP_E:\n",
    "                if main_proc: print(\"[Epoch] Unfreezing encoder\", flush=True)\n",
    "                if ddp:\n",
    "                    for p in model.module.enc.parameters():\n",
    "                        p.requires_grad = True\n",
    "                else:\n",
    "                    for p in model.enc.parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "            # Train\n",
    "            model.train()\n",
    "            tot_loss = 0.0\n",
    "            for b in ltr:\n",
    "                ids = b['input_ids'].to(dev, non_blocking=True)\n",
    "                m = b['attention_mask'].to(dev, non_blocking=True)\n",
    "                num = b['num'].to(dev, non_blocking=True)\n",
    "                u = b['unit'].to(dev, non_blocking=True)\n",
    "                c = b['cat'].to(dev, non_blocking=True)\n",
    "                f = b['flav'].to(dev, non_blocking=True)\n",
    "                y_log = b['ylog'].to(dev, non_blocking=True)\n",
    "                y_raw = b['y_raw'].to(dev, non_blocking=True)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.cuda.amp.autocast(enabled=(dev.type == 'cuda')):\n",
    "                    pr_log = model(ids, m, num, u, c, f)\n",
    "                    pr_raw = torch.expm1(pr_log).clamp(min=0.99)\n",
    "                    loss = combined_loss(pr_log, y_log, pr_raw, y_raw, alpha=ALPHA)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                sched.step()\n",
    "\n",
    "                tot_loss += float(loss.item())\n",
    "                if tbar is not None: tbar.update(1)\n",
    "\n",
    "            if tbar is not None: tbar.close()\n",
    "            # Reduce train loss across ranks\n",
    "            tl = torch.tensor([tot_loss, len(ltr)], dtype=torch.float32, device=dev)\n",
    "            if ddp:\n",
    "                dist.all_reduce(tl, op=dist.ReduceOp.SUM)\n",
    "            tr_loss = (tl[0].item() / max(1.0, tl[1].item()))\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            P_local = []\n",
    "            T_local = []\n",
    "            with torch.no_grad():\n",
    "                if main_proc:\n",
    "                    vbar = tqdm(total=len(lva), desc=\"val\", mininterval=1.0, leave=False)\n",
    "                else:\n",
    "                    vbar = None\n",
    "\n",
    "                for b in lva:\n",
    "                    ids = b['input_ids'].to(dev, non_blocking=True)\n",
    "                    m  = b['attention_mask'].to(dev, non_blocking=True)\n",
    "                    num = b['num'].to(dev, non_blocking=True)\n",
    "                    u = b['unit'].to(dev, non_blocking=True)\n",
    "                    c = b['cat'].to(dev, non_blocking=True)\n",
    "                    f = b['flav'].to(dev, non_blocking=True)\n",
    "                    y_log = b['ylog'].to(dev, non_blocking=True)\n",
    "\n",
    "                    with torch.cuda.amp.autocast(enabled=(dev.type == 'cuda')):\n",
    "                        pr_log = model(ids, m, num, u, c, f)\n",
    "\n",
    "                    P_local.append(pr_log.float().cpu().numpy())\n",
    "                    T_local.append(y_log.float().cpu().numpy())\n",
    "                    if vbar is not None: vbar.update(1)\n",
    "\n",
    "                if vbar is not None: vbar.close()\n",
    "\n",
    "            P_local = np.concatenate(P_local) if len(P_local) else np.array([], dtype=np.float32)\n",
    "            T_local = np.concatenate(T_local) if len(T_local) else np.array([], dtype=np.float32)\n",
    "\n",
    "            # Gather predictions/targets to rank 0\n",
    "            if ddp:\n",
    "                P_all = all_gather_numpy(P_local)\n",
    "                T_all = all_gather_numpy(T_local)\n",
    "            else:\n",
    "                P_all, T_all = P_local, T_local\n",
    "\n",
    "            if main_proc:\n",
    "                sm = smape_np(np.expm1(P_all).clip(0.99, None), np.expm1(T_all))\n",
    "                print(f\"[Epoch {ep+1}] train loss: {tr_loss:.4f} | val SMAPE: {sm:.2f}%\", flush=True)\n",
    "\n",
    "                # Save best\n",
    "                if sm + 1e-6 < best:\n",
    "                    best = sm\n",
    "                    state = model.module.state_dict() if ddp else model.state_dict()\n",
    "                    torch.save({\n",
    "                        'state_dict': state,\n",
    "                        'unit_classes': dtr.unit_le.classes_.tolist(),\n",
    "                        'cat_classes': dtr.cat_le.classes_.tolist(),\n",
    "                        'flav_classes': dtr.flav_le.classes_.tolist(),\n",
    "                        'scaler_mean': dtr.scaler.mean_.tolist(),\n",
    "                        'scaler_scale': dtr.scaler.scale_.tolist(),\n",
    "                        'numeric_cols': dtr.numeric_cols,\n",
    "                        'tokenizer': MODEL_NAME,\n",
    "                        'best_smape': best,\n",
    "                        'max_length': MAXL,\n",
    "                        'hidden_dim': 768,\n",
    "                        'alpha': ALPHA,\n",
    "                    }, os.path.join(OUT, \"best_model.pt\"))\n",
    "                    print(f\"[Save] Checkpoint saved (SMAPE {best:.2f}%) → {OUT}/best_model.pt\", flush=True)\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= patience:\n",
    "                        print(\"[EarlyStop] Patience exceeded. Stopping.\", flush=True)\n",
    "                        break\n",
    "\n",
    "        if main_proc:\n",
    "            print(f\"\\n[Done] Best val SMAPE: {best:.2f}% | Artifacts at {OUT}\", flush=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        if main_proc:\n",
    "            print(\"[Fatal] Unhandled exception:\", repr(e), flush=True)\n",
    "            traceback.print_exc()\n",
    "        # let it raise — no sys.exit to avoid IPython traceback noise\n",
    "        raise\n",
    "    finally:\n",
    "        if ddp:\n",
    "            dist.barrier()\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# %% ===================== Entrypoint =====================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
