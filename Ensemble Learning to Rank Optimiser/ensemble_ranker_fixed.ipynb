{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ› ï¸ Install\n",
    "%pip install -q --upgrade xgboost lightgbm catboost polars pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, gc, pickle, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from typing import List, Optional, Dict\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def dcg_at_k(relevances, k=3):\n",
    "    rels = np.asfarray(relevances)[:k]\n",
    "    if rels.size:\n",
    "        discounts = np.log2(np.arange(2, rels.size + 2))\n",
    "        return np.sum((np.power(2, rels) - 1) / discounts)\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(true_rels, pred_scores, k=3):\n",
    "    order = np.argsort(-pred_scores)\n",
    "    ideal = dcg_at_k(sorted(true_rels, reverse=True), k)\n",
    "    if ideal == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(np.array(true_rels)[order], k) / ideal\n",
    "\n",
    "def hitrate_at_k(true_rels, pred_scores, k=3):\n",
    "    order = np.argsort(-pred_scores)[:k]\n",
    "    return 1.0 if np.any(np.array(true_rels)[order] > 0) else 0.0\n",
    "\n",
    "def groupwise_metric(groups, y_true, y_pred, k=3):\n",
    "    start = 0\n",
    "    ndcgs, hits = [], []\n",
    "    for g in groups:\n",
    "        end = start + g\n",
    "        rels = y_true[start:end]\n",
    "        scores = y_pred[start:end]\n",
    "        ndcgs.append(ndcg_at_k(rels, scores, k=k))\n",
    "        hits.append(hitrate_at_k(rels, scores, k=k))\n",
    "        start = end\n",
    "    return float(np.mean(ndcgs) if ndcgs else 0.0), float(np.mean(hits) if hits else 0.0)\n",
    "\n",
    "def find_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "        for col in df.columns:\n",
    "            if col.lower() == c.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def load_data(train_path=\"train.parquet\", test_path=\"test.parquet\", features_pickle=\"features.pkl\") -> Dict:\n",
    "    if Path(train_path).suffix.lower() in [\".parquet\", \".pq\"]:\n",
    "        train_df = pl.read_parquet(train_path).to_pandas()\n",
    "    else:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "    test_df = None\n",
    "    if test_path and Path(test_path).exists():\n",
    "        if Path(test_path).suffix.lower() in [\".parquet\", \".pq\"]:\n",
    "            test_df = pl.read_parquet(test_path).to_pandas()\n",
    "        else:\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "    feature_cols = None\n",
    "    cat_features = []\n",
    "    if Path(features_pickle).exists():\n",
    "        try:\n",
    "            with open(features_pickle, \"rb\") as f:\n",
    "                meta = pickle.load(f)\n",
    "            feature_cols = meta.get(\"feature_cols\")\n",
    "            cat_features = meta.get(\"cat_features_final\", [])\n",
    "            print(f\"Loaded {features_pickle}: {len(feature_cols or [])} features, {len(cat_features)} categorical.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning loading {features_pickle}: {e}\")\n",
    "\n",
    "    target_col = find_column(train_df, [\"label\",\"target\",\"relevance\",\"clicked\",\"y\"])\n",
    "    group_col  = find_column(train_df, [\"ranker_id\",\"query_id\",\"session_id\",\"qid\",\"search_id\"])\n",
    "    id_col     = find_column(train_df, [\"id\",\"row_id\",\"pair_id\",\"doc_id\",\"item_id\"])\n",
    "\n",
    "    if target_col is None or group_col is None:\n",
    "        raise ValueError(f\"Need label & group columns. Found label={target_col}, group={group_col}.\")\n",
    "\n",
    "    if feature_cols is None:\n",
    "        exclude = {target_col, group_col}\n",
    "        if id_col: exclude.add(id_col)\n",
    "        feature_cols = [c for c in train_df.columns if c not in exclude and train_df[c].dtype != 'O']\n",
    "        obj_cols = [c for c in train_df.columns if train_df[c].dtype == 'O' and c not in exclude]\n",
    "        low_card = [c for c in obj_cols if train_df[c].nunique(dropna=False) <= 1000]\n",
    "        cat_features = low_card\n",
    "        feature_cols += low_card\n",
    "        print(f\"Inferred {len(feature_cols)} features ({len(cat_features)} categorical).\")\n",
    "\n",
    "    return dict(train=train_df, test=test_df, features=feature_cols, cat_features=cat_features,\n",
    "                target=target_col, group=group_col, id=id_col)\n",
    "\n",
    "def group_split(df: pd.DataFrame, group_col: str, test_size=0.2, seed=42):\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    groups = df[group_col].values\n",
    "    idx_tr, idx_va = next(splitter.split(df, groups=groups, y=df[group_col].values))\n",
    "    return idx_tr, idx_va\n",
    "\n",
    "print(\"âœ… Imports & utils ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ Prepare data\n",
    "TRAIN_PATH = \"train.parquet\"   # or CSV\n",
    "TEST_PATH  = \"test.parquet\"    # or CSV or None\n",
    "FEATURES_PKL = \"features.pkl\"  # optional\n",
    "\n",
    "data = load_data(TRAIN_PATH, TEST_PATH, FEATURES_PKL)\n",
    "\n",
    "df = data[\"train\"].copy()\n",
    "test_df = data[\"test\"]\n",
    "feature_cols = data[\"features\"]\n",
    "cat_features = data[\"cat_features\"]\n",
    "target_col = data[\"target\"]\n",
    "group_col = data[\"group\"]\n",
    "id_col = data[\"id\"]\n",
    "\n",
    "for c in cat_features:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "        if test_df is not None and c in test_df.columns:\n",
    "            test_df[c] = test_df[c].astype(\"category\")\n",
    "\n",
    "idx_tr, idx_va = group_split(df, group_col, test_size=0.2, seed=42)\n",
    "train_df = df.iloc[idx_tr].reset_index(drop=True)\n",
    "val_df   = df.iloc[idx_va].reset_index(drop=True)\n",
    "\n",
    "def to_group_sizes(g):\n",
    "    _, counts = np.unique(g, return_counts=True)\n",
    "    return counts\n",
    "\n",
    "train_groups = to_group_sizes(train_df[group_col].values)\n",
    "val_groups   = to_group_sizes(val_df[group_col].values)\n",
    "\n",
    "X_tr = train_df[feature_cols]; y_tr = train_df[target_col].values\n",
    "X_va = val_df[feature_cols];   y_va = val_df[target_col].values\n",
    "X_te = test_df[feature_cols] if test_df is not None else None\n",
    "\n",
    "print(f\"Train: {df.shape} | Train groups: {len(train_groups)} | Val groups: {len(val_groups)}\")\n",
    "print(\"âœ… Data ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸš€ Model 1 â€” XGBoost Ranker\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"rank:pairwise\",\n",
    "    eval_metric=\"ndcg@3\",\n",
    "    tree_method=\"hist\",\n",
    "    learning_rate=0.08,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr); dtrain.set_group(train_groups)\n",
    "dvalid = xgb.DMatrix(X_va, label=y_va); dvalid.set_group(val_groups)\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain,\"train\"),(dvalid,\"valid\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50,\n",
    ")\n",
    "\n",
    "xgb_va_pred = xgb_model.predict(dvalid, iteration_range=(0, xgb_model.best_iteration+1))\n",
    "ndcg3, hit3 = groupwise_metric(val_groups, y_va, xgb_va_pred, k=3)\n",
    "print(f\"NDCG@3={ndcg3:.5f} | HitRate@3={hit3:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸŒ¿ Model 2 â€” LightGBM LambdaRank\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = dict(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[3],\n",
    "    learning_rate=0.08,\n",
    "    num_leaves=127,\n",
    "    min_data_in_leaf=50,\n",
    "    feature_fraction=0.85,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_lambda=1.0,\n",
    "    verbosity=-1,\n",
    "    deterministic=True,\n",
    "    force_row_wise=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr, group=train_groups, free_raw_data=False)\n",
    "lgb_valid = lgb.Dataset(X_va, label=y_va, group=val_groups, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    params=lgb_params,\n",
    "    train_set=lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    valid_names=[\"train\",\"valid\"],\n",
    "    num_boost_round=5000,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=100,\n",
    ")\n",
    "\n",
    "lgb_va_pred = lgb_model.predict(X_va, num_iteration=lgb_model.best_iteration)\n",
    "ndcg3, hit3 = groupwise_metric(val_groups, y_va, lgb_va_pred, k=3)\n",
    "print(f\"NDCG@3={ndcg3:.5f} | HitRate@3={hit3:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸˆ Model 3 â€” CatBoost Ranker\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "cat_idx = [i for i, c in enumerate(feature_cols) if c in cat_features]\n",
    "\n",
    "train_pool = Pool(X_tr, label=y_tr, group_id=train_df[group_col].values, cat_features=cat_idx or None)\n",
    "valid_pool = Pool(X_va, label=y_va, group_id=val_df[group_col].values, cat_features=cat_idx or None)\n",
    "\n",
    "cb_model = CatBoostRanker(\n",
    "    loss_function=\"YetiRank\",\n",
    "    eval_metric=\"NDCG:top=3\",\n",
    "    learning_rate=0.08,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=42,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    "    iterations=5000,\n",
    "    verbose=100,\n",
    "    task_type=\"CPU\",\n",
    ")\n",
    "\n",
    "cb_model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "cb_va_pred = cb_model.predict(valid_pool)\n",
    "ndcg3, hit3 = groupwise_metric(val_groups, y_va, cb_va_pred, k=3)\n",
    "print(f\"NDCG@3={ndcg3:.5f} | HitRate@3={hit3:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
