{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHEagQ7KZq1T"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import cupy as cp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data processing and ML\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from pmdarima import auto_arima\n",
        "\n",
        "# Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import requests\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm7thnY6Z4tQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"PyTorch LSTM Model for stock prediction\"\"\"\n",
        "    def __init__(self, input_size, lstm_units=50, num_layers=3, dropout_rate=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.lstm_units = lstm_units\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=lstm_units,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        # Dense layers\n",
        "        self.fc1 = nn.Linear(lstm_units, 25)\n",
        "        self.fc2 = nn.Linear(25, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.lstm_units).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.lstm_units).to(x.device)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        # Take the output from the last time step\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        \n",
        "        # Apply dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        \n",
        "        # Dense layers\n",
        "        out = self.relu(self.fc1(lstm_out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class StockForecasterPyTorch:\n",
        "    def __init__(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        self.data = None\n",
        "        self.arima_model = None\n",
        "        self.lstm_model = None\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        self.arima_residuals = None\n",
        "        self.predictions = None\n",
        "        self.news_api_key = \"02cc51bf98a740e19221e22ef87f6d56\"  # Replace with your News API key\n",
        "        self.device = device\n",
        "        \n",
        "        # CuPy arrays for GPU acceleration\n",
        "        self.use_cupy = cp.cuda.is_available()\n",
        "        if self.use_cupy:\n",
        "            print(\"CuPy GPU acceleration enabled\")\n",
        "        else:\n",
        "            print(\"CuPy not available, using CPU\")\n",
        "\n",
        "    def cupy_to_numpy(self, arr):\n",
        "        \"\"\"Convert CuPy array to NumPy if needed\"\"\"\n",
        "        if self.use_cupy and isinstance(arr, cp.ndarray):\n",
        "            return cp.asnumpy(arr)\n",
        "        return arr\n",
        "\n",
        "    def numpy_to_cupy(self, arr):\n",
        "        \"\"\"Convert NumPy array to CuPy if possible\"\"\"\n",
        "        if self.use_cupy and isinstance(arr, np.ndarray):\n",
        "            return cp.asarray(arr)\n",
        "        return arr\n",
        "\n",
        "    def fetch_stock_data(self, period=\"2y\"):\n",
        "        \"\"\"Fetch stock data from Yahoo Finance\"\"\"\n",
        "        try:\n",
        "            ticker = yf.Ticker(self.symbol)\n",
        "            self.data = ticker.history(period=period)\n",
        "\n",
        "            # Add technical indicators using CuPy for acceleration\n",
        "            close_prices = self.numpy_to_cupy(self.data['Close'].values)\n",
        "            \n",
        "            # Moving averages\n",
        "            ma_20 = self.cupy_to_numpy(self.calculate_ma_cupy(close_prices, 20))\n",
        "            ma_50 = self.cupy_to_numpy(self.calculate_ma_cupy(close_prices, 50))\n",
        "            \n",
        "            self.data['MA_20'] = ma_20\n",
        "            self.data['MA_50'] = ma_50\n",
        "            self.data['RSI'] = self.calculate_rsi(self.data['Close'])\n",
        "            \n",
        "            # Volatility using CuPy\n",
        "            returns = cp.diff(close_prices) / close_prices[:-1] if self.use_cupy else np.diff(close_prices) / close_prices[:-1]\n",
        "            volatility = self.cupy_to_numpy(self.rolling_std_cupy(returns, 20)) if self.use_cupy else pd.Series(returns).rolling(20).std().values\n",
        "            self.data['Volatility'] = np.concatenate([[np.nan], volatility])\n",
        "\n",
        "            print(f\"Fetched {len(self.data)} days of data for {self.symbol}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            return False\n",
        "\n",
        "    def calculate_ma_cupy(self, prices, window):\n",
        "        \"\"\"Calculate moving average using CuPy\"\"\"\n",
        "        if not self.use_cupy:\n",
        "            return pd.Series(prices).rolling(window).mean().values\n",
        "        \n",
        "        ma = cp.full_like(prices, cp.nan)\n",
        "        for i in range(window - 1, len(prices)):\n",
        "            ma[i] = cp.mean(prices[i - window + 1:i + 1])\n",
        "        return ma\n",
        "\n",
        "    def rolling_std_cupy(self, arr, window):\n",
        "        \"\"\"Calculate rolling standard deviation using CuPy\"\"\"\n",
        "        if not self.use_cupy:\n",
        "            return pd.Series(arr).rolling(window).std().values\n",
        "        \n",
        "        std = cp.full(len(arr) - window + 1, cp.nan)\n",
        "        for i in range(window - 1, len(arr)):\n",
        "            std[i - window + 1] = cp.std(arr[i - window + 1:i + 1])\n",
        "        return std\n",
        "\n",
        "    def calculate_rsi(self, prices, window=14):\n",
        "        \"\"\"Calculate RSI indicator\"\"\"\n",
        "        delta = prices.diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi\n",
        "\n",
        "    def fetch_news_sentiment(self, days_back=7):\n",
        "        \"\"\"Fetch news and calculate sentiment score\"\"\"\n",
        "        try:\n",
        "            end_date = datetime.now()\n",
        "            start_date = end_date - timedelta(days=days_back)\n",
        "\n",
        "            url = f\"https://newsapi.org/v2/everything\"\n",
        "            params = {\n",
        "                'q': f\"{self.symbol} OR stock market OR economy\",\n",
        "                'from': start_date.strftime('%Y-%m-%d'),\n",
        "                'to': end_date.strftime('%Y-%m-%d'),\n",
        "                'sortBy': 'relevancy',\n",
        "                'apiKey': self.news_api_key,\n",
        "                'language': 'en',\n",
        "                'pageSize': 50\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params)\n",
        "            news_data = response.json()\n",
        "\n",
        "            if news_data['status'] == 'ok':\n",
        "                articles = news_data['articles']\n",
        "                sentiments = []\n",
        "                analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "                for article in articles:\n",
        "                    title = article.get('title', '')\n",
        "                    description = article.get('description', '')\n",
        "                    text = f\"{title} {description}\"\n",
        "\n",
        "                    # VADER sentiment\n",
        "                    vader_score = analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "                    # TextBlob sentiment\n",
        "                    blob = TextBlob(text)\n",
        "                    textblob_score = blob.sentiment.polarity\n",
        "\n",
        "                    # Average sentiment\n",
        "                    avg_sentiment = (vader_score + textblob_score) / 2\n",
        "                    sentiments.append(avg_sentiment)\n",
        "\n",
        "                overall_sentiment = np.mean(sentiments) if sentiments else 0\n",
        "                print(f\"News sentiment score: {overall_sentiment:.3f} (from {len(articles)} articles)\")\n",
        "                return overall_sentiment, articles[:10]  # Return top 10 articles\n",
        "            else:\n",
        "                print(\"No news data available\")\n",
        "                return 0, []\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching news sentiment: {e}\")\n",
        "            return 0, []\n",
        "\n",
        "    def optimize_arima_parameters(self, max_p=5, max_d=2, max_q=5):\n",
        "        \"\"\"Optimize ARIMA parameters using grid search and information criteria\"\"\"\n",
        "        print(\"Optimizing ARIMA hyperparameters...\")\n",
        "\n",
        "        # Check stationarity first\n",
        "        from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "        ts_data = self.data['Close'].dropna()\n",
        "        adf_result = adfuller(ts_data)\n",
        "        print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n",
        "        print(f\"p-value: {adf_result[1]:.6f}\")\n",
        "\n",
        "        is_stationary = adf_result[1] <= 0.05\n",
        "        print(f\"Series is {'stationary' if is_stationary else 'non-stationary'}\")\n",
        "\n",
        "        # Grid search for optimal parameters\n",
        "        best_aic = float('inf')\n",
        "        best_bic = float('inf')\n",
        "        best_order_aic = None\n",
        "        best_order_bic = None\n",
        "        results = []\n",
        "\n",
        "        print(\"\\nGrid search for optimal ARIMA parameters:\")\n",
        "        print(\"p\\td\\tq\\tAIC\\t\\tBIC\\t\\tHQIC\\t\\tLLF\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for p in range(0, max_p + 1):\n",
        "            for d in range(0, max_d + 1):\n",
        "                for q in range(0, max_q + 1):\n",
        "                    try:\n",
        "                        # Skip if d is too high for stationary series\n",
        "                        if is_stationary and d > 1:\n",
        "                            continue\n",
        "\n",
        "                        # Fit ARIMA model\n",
        "                        model = ARIMA(ts_data, order=(p, d, q))\n",
        "                        fitted_model = model.fit()\n",
        "\n",
        "                        aic = fitted_model.aic\n",
        "                        bic = fitted_model.bic\n",
        "                        hqic = fitted_model.hqic\n",
        "                        llf = fitted_model.llf\n",
        "\n",
        "                        results.append({\n",
        "                            'order': (p, d, q),\n",
        "                            'aic': aic,\n",
        "                            'bic': bic,\n",
        "                            'hqic': hqic,\n",
        "                            'llf': llf,\n",
        "                            'model': fitted_model\n",
        "                        })\n",
        "\n",
        "                        print(f\"{p}\\t{d}\\t{q}\\t{aic:.4f}\\t\\t{bic:.4f}\\t\\t{hqic:.4f}\\t\\t{llf:.4f}\")\n",
        "\n",
        "                        # Track best models\n",
        "                        if aic < best_aic:\n",
        "                            best_aic = aic\n",
        "                            best_order_aic = (p, d, q)\n",
        "\n",
        "                        if bic < best_bic:\n",
        "                            best_bic = bic\n",
        "                            best_order_bic = (p, d, q)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "        # Select best model based on AIC (generally preferred for forecasting)\n",
        "        best_result = min(results, key=lambda x: x['aic'])\n",
        "        best_order = best_result['order']\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Optimization Results:\")\n",
        "        print(f\"Best order by AIC: {best_order_aic} (AIC: {best_aic:.4f})\")\n",
        "        print(f\"Best order by BIC: {best_order_bic} (BIC: {best_bic:.4f})\")\n",
        "        print(f\"Selected order: {best_order}\")\n",
        "\n",
        "        return best_result['model'], best_order, results\n",
        "\n",
        "    def fit_arima(self):\n",
        "        \"\"\"Fit ARIMA model to capture linear trends with hyperparameter optimization\"\"\"\n",
        "        try:\n",
        "            print(\"Fitting ARIMA model with hyperparameter optimization...\")\n",
        "\n",
        "            # First try automated approach\n",
        "            try:\n",
        "                print(\"\\n1ï¸âƒ£ Trying auto_arima for quick optimization...\")\n",
        "                auto_model = auto_arima(\n",
        "                    self.data['Close'].dropna(),\n",
        "                    start_p=0, start_q=0, start_P=0, start_Q=0,\n",
        "                    test='adf',\n",
        "                    max_p=5, max_q=5, max_P=2, max_Q=2,\n",
        "                    seasonal=True, m=12,  # Monthly seasonality\n",
        "                    stepwise=True,\n",
        "                    suppress_warnings=True,\n",
        "                    error_action='ignore',\n",
        "                    trace=True,\n",
        "                    information_criterion='aic',\n",
        "                    out_of_sample_size=int(len(self.data) * 0.2)  # 20% for validation\n",
        "                )\n",
        "\n",
        "                auto_order = auto_model.order\n",
        "                auto_seasonal_order = auto_model.seasonal_order if hasattr(auto_model, 'seasonal_order') else (0, 0, 0, 0)\n",
        "                auto_aic = auto_model.aic()\n",
        "\n",
        "                print(f\"Auto-ARIMA order: {auto_order}\")\n",
        "                print(f\"Auto-ARIMA seasonal order: {auto_seasonal_order}\")\n",
        "                print(f\"Auto-ARIMA AIC: {auto_aic:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Auto-ARIMA failed: {e}\")\n",
        "                auto_model = None\n",
        "                auto_aic = float('inf')\n",
        "\n",
        "            # Manual grid search optimization\n",
        "            print(\"\\n2ï¸âƒ£ Manual grid search optimization...\")\n",
        "            manual_model, manual_order, all_results = self.optimize_arima_parameters()\n",
        "            manual_aic = manual_model.aic\n",
        "\n",
        "            # Compare and select best model\n",
        "            if auto_model is not None and auto_aic < manual_aic:\n",
        "                print(f\"\\nâœ… Selected: Auto-ARIMA (AIC: {auto_aic:.4f})\")\n",
        "                self.arima_model = ARIMA(\n",
        "                    self.data['Close'].dropna(),\n",
        "                    order=auto_order,\n",
        "                    seasonal_order=auto_seasonal_order\n",
        "                ).fit()\n",
        "                final_order = auto_order\n",
        "            else:\n",
        "                print(f\"\\nâœ… Selected: Manual optimization (AIC: {manual_aic:.4f})\")\n",
        "                self.arima_model = manual_model\n",
        "                final_order = manual_order\n",
        "\n",
        "            # Validate model diagnostics\n",
        "            self.validate_arima_model()\n",
        "\n",
        "            # Get residuals for LSTM\n",
        "            fitted_values = self.arima_model.fittedvalues\n",
        "            self.arima_residuals = self.data['Close'].dropna() - fitted_values\n",
        "\n",
        "            # Calculate residual statistics\n",
        "            residual_mean = self.arima_residuals.mean()\n",
        "            residual_std = self.arima_residuals.std()\n",
        "\n",
        "            print(f\"\\nðŸ“Š Final ARIMA Model Summary:\")\n",
        "            print(f\"Order: {final_order}\")\n",
        "            print(f\"AIC: {self.arima_model.aic:.4f}\")\n",
        "            print(f\"BIC: {self.arima_model.bic:.4f}\")\n",
        "            print(f\"Log-likelihood: {self.arima_model.llf:.4f}\")\n",
        "            print(f\"Residual mean: {residual_mean:.6f}\")\n",
        "            print(f\"Residual std: {residual_std:.4f}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting ARIMA: {e}\")\n",
        "            return False\n",
        "\n",
        "    def validate_arima_model(self):\n",
        "        \"\"\"Validate ARIMA model with diagnostic tests\"\"\"\n",
        "        try:\n",
        "            from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "            from statsmodels.stats.stattools import jarque_bera\n",
        "\n",
        "            residuals = self.arima_model.resid\n",
        "\n",
        "            # Ljung-Box test for autocorrelation in residuals\n",
        "            lb_stat, lb_pvalue = acorr_ljungbox(residuals, lags=10, return_df=False)\n",
        "\n",
        "            # Jarque-Bera test for normality\n",
        "            jb_stat, jb_pvalue, _, _ = jarque_bera(residuals)\n",
        "            print(f\"\\nðŸ” ARIMA Model Diagnostics:\")\n",
        "            lb_pval = float(lb_pvalue[-1]) if lb_pvalue[-1] is not None else float('nan')\n",
        "            jb_pval = float(jb_pvalue) if jb_pvalue is not None else float('nan')\n",
        "\n",
        "            print(f\"Ljung-Box test (autocorrelation): p-value = {lb_pval:.4f}\")\n",
        "            print(f\"{'âœ… No autocorrelation' if lb_pval > 0.05 else 'âš ï¸ Autocorrelation detected'}\")\n",
        "\n",
        "            print(f\"Jarque-Bera test (normality): p-value = {jb_pval:.4f}\")\n",
        "            print(f\"{'âœ… Residuals are normal' if jb_pval > 0.05 else 'âš ï¸ Residuals are not normal'}\")\n",
        "\n",
        "            return {\n",
        "                'ljung_box_pvalue': lb_pvalue[-1],\n",
        "                'jarque_bera_pvalue': jb_pvalue,\n",
        "                'autocorrelation_ok': lb_pvalue[-1] > 0.05,\n",
        "                'normality_ok': jb_pvalue > 0.05\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in model validation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def prepare_lstm_data(self, look_back=60):\n",
        "        \"\"\"Prepare data for LSTM training using CuPy acceleration\"\"\"\n",
        "        try:\n",
        "            # Combine residuals with technical indicators\n",
        "            features = pd.DataFrame({\n",
        "                'residuals': self.arima_residuals,\n",
        "                'volume': self.data['Volume'][self.arima_residuals.index],\n",
        "                'rsi': self.data['RSI'][self.arima_residuals.index],\n",
        "                'volatility': self.data['Volatility'][self.arima_residuals.index]\n",
        "            }).fillna(method='ffill').dropna()\n",
        "\n",
        "            # Scale features\n",
        "            scaled_features = self.scaler.fit_transform(features)\n",
        "\n",
        "            # Use CuPy for sequence creation if available\n",
        "            if self.use_cupy:\n",
        "                scaled_features_gpu = cp.asarray(scaled_features)\n",
        "                X_gpu = cp.zeros((len(scaled_features) - look_back, look_back, scaled_features.shape[1]))\n",
        "                y_gpu = cp.zeros(len(scaled_features) - look_back)\n",
        "                \n",
        "                for i in range(look_back, len(scaled_features)):\n",
        "                    X_gpu[i - look_back] = scaled_features_gpu[i - look_back:i]\n",
        "                    y_gpu[i - look_back] = scaled_features_gpu[i, 0]  # Predict residuals\n",
        "                \n",
        "                X = cp.asnumpy(X_gpu)\n",
        "                y = cp.asnumpy(y_gpu)\n",
        "            else:\n",
        "                # Create sequences using NumPy\n",
        "                X, y = [], []\n",
        "                for i in range(look_back, len(scaled_features)):\n",
        "                    X.append(scaled_features[i-look_back:i])\n",
        "                    y.append(scaled_features[i, 0])  # Predict residuals\n",
        "                X, y = np.array(X), np.array(y)\n",
        "\n",
        "            return X, y\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing LSTM data: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def optimize_lstm_hyperparameters(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Optimize LSTM hyperparameters using PyTorch\"\"\"\n",
        "        print(\"Optimizing LSTM hyperparameters...\")\n",
        "\n",
        "        # Hyperparameter grid\n",
        "        param_grid = {\n",
        "            'lstm_units': [32, 50, 64, 100],\n",
        "            'dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'learning_rate': [0.001, 0.01, 0.1],\n",
        "            'batch_size': [16, 32, 64],\n",
        "            'num_layers': [2, 3, 4]\n",
        "        }\n",
        "\n",
        "        best_score = float('inf')\n",
        "        best_params = None\n",
        "        results = []\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
        "        y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(self.device)\n",
        "        X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
        "        y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1)).to(self.device)\n",
        "\n",
        "        # Random search (to avoid exhaustive grid search)\n",
        "        import random\n",
        "        n_trials = 20  # Number of random combinations to try\n",
        "\n",
        "        print(\"Trying random hyperparameter combinations:\")\n",
        "        print(\"Trial\\tUnits\\tLayers\\tDropout\\tLR\\tBatch\\tVal Loss\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for trial in range(n_trials):\n",
        "            params = {\n",
        "                'lstm_units': random.choice(param_grid['lstm_units']),\n",
        "                'dropout_rate': random.choice(param_grid['dropout_rate']),\n",
        "                'learning_rate': random.choice(param_grid['learning_rate']),\n",
        "                'batch_size': random.choice(param_grid['batch_size']),\n",
        "                'num_layers': random.choice(param_grid['num_layers'])\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Build model with current parameters\n",
        "                model = LSTMModel(\n",
        "                    input_size=X_train.shape[2],\n",
        "                    lstm_units=params['lstm_units'],\n",
        "                    num_layers=params['num_layers'],\n",
        "                    dropout_rate=params['dropout_rate']\n",
        "                ).to(self.device)\n",
        "\n",
        "                # Define loss and optimizer\n",
        "                criterion = nn.MSELoss()\n",
        "                optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "                # Create data loaders\n",
        "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "                # Train model\n",
        "                model.train()\n",
        "                for epoch in range(50):  # Reduced epochs for hyperparameter search\n",
        "                    epoch_loss = 0\n",
        "                    for batch_X, batch_y in train_loader:\n",
        "                        optimizer.zero_grad()\n",
        "                        outputs = model(batch_X)\n",
        "                        loss = criterion(outputs, batch_y)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        epoch_loss += loss.item()\n",
        "\n",
        "                # Evaluate on validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_val_tensor)\n",
        "                    val_loss = criterion(val_outputs, y_val_tensor).item()\n",
        "\n",
        "                results.append({\n",
        "                    'params': params.copy(),\n",
        "                    'val_loss': val_loss,\n",
        "                    'model': model\n",
        "                })\n",
        "\n",
        "                print(f\"{trial+1:2d}\\t{params['lstm_units']:3d}\\t{params['num_layers']}\\t{params['dropout_rate']:.1f}\\t{params['learning_rate']:.3f}\\t{params['batch_size']:2d}\\t{val_loss:.6f}\")\n",
        "\n",
        "                if val_loss < best_score:\n",
        "                    best_score = val_loss\n",
        "                    best_params = params.copy()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{trial+1:2d}\\tFAILED: {str(e)[:30]}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Best LSTM Parameters:\")\n",
        "        for key, value in best_params.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "        print(f\"Best validation loss: {best_score:.6f}\")\n",
        "\n",
        "        return best_params, results\n",
        "\n",
        "    def train_lstm(self, epochs=100, batch_size=32):\n",
        "        \"\"\"Train LSTM model on residuals with PyTorch and hyperparameter optimization\"\"\"\n",
        "        try:\n",
        "            print(\"Preparing LSTM data...\")\n",
        "            X, y = self.prepare_lstm_data()\n",
        "\n",
        "            if X is None or y is None:\n",
        "                return False\n",
        "\n",
        "            # Split data\n",
        "            split_idx = int(len(X) * 0.7)  # 70% train\n",
        "            val_split_idx = int(len(X) * 0.85)  # 15% validation, 15% test\n",
        "\n",
        "            X_train, y_train = X[:split_idx], y[:split_idx]\n",
        "            X_val, y_val = X[split_idx:val_split_idx], y[split_idx:val_split_idx]\n",
        "            X_test, y_test = X[val_split_idx:], y[val_split_idx:]\n",
        "\n",
        "            print(f\"Training samples: {len(X_train)}\")\n",
        "            print(f\"Validation samples: {len(X_val)}\")\n",
        "            print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "            # Optimize hyperparameters\n",
        "            best_params, optimization_results = self.optimize_lstm_hyperparameters(\n",
        "                X_train, y_train, X_val, y_val\n",
        "            )\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            print(f\"\\nðŸ”„ Training final LSTM model with best parameters...\")\n",
        "            self.lstm_model = LSTMModel(\n",
        "                input_size=X.shape[2],\n",
        "                lstm_units=best_params['lstm_units'],\n",
        "                num_layers=best_params['num_layers'],\n",
        "                dropout_rate=best_params['dropout_rate']\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
        "            y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(self.device)\n",
        "            X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
        "            y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1)).to(self.device)\n",
        "            X_test_tensor = torch.FloatTensor(X_test).to(self.device)\n",
        "            y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(self.device)\n",
        "\n",
        "            # Define loss and optimizer\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(self.lstm_model.parameters(), lr=best_params['learning_rate'])\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, min_lr=1e-7)\n",
        "\n",
        "            # Create data loaders\n",
        "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "\n",
        "            # Training loop with early stopping\n",
        "            best_val_loss = float('inf')\n",
        "            patience_counter = 0\n",
        "            patience = 15\n",
        "            training_history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                # Training phase\n",
        "                self.lstm_model.train()\n",
        "                train_loss = 0\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = self.lstm_model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    train_loss += loss.item()\n",
        "\n",
        "                train_loss /= len(train_loader)\n",
        "\n",
        "                # Validation phase\n",
        "                self.lstm_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = self.lstm_model(X_val_tensor)\n",
        "                    val_loss = criterion(val_outputs, y_val_tensor).item()\n",
        "\n",
        "                # Learning rate scheduling\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                training_history['train_loss'].append(train_loss)\n",
        "                training_history['val_loss'].append(val_loss)\n",
        "\n",
        "                # Early stopping\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    # Save best model\n",
        "                    torch.save(self.lstm_model.state_dict(), 'best_lstm_model.pth')\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "            # Load best model\n",
        "            self.lstm_model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
        "\n",
        "            # Final evaluation\n",
        "            self.lstm_model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_outputs = self.lstm_model(X_train_tensor)\n",
        "                train_loss = criterion(train_outputs, y_train_tensor).item()\n",
        "                train_mae = torch.mean(torch.abs(train_outputs - y_train_tensor)).item()\n",
        "\n",
        "                val_outputs = self.lstm_model(X_val_tensor)\n",
        "                val_loss = criterion(val_outputs, y_val_tensor).item()\n",
        "                val_mae = torch.mean(torch.abs(val_outputs - y_val_tensor)).item()\n",
        "\n",
        "                test_outputs = self.lstm_model(X_test_tensor)\n",
        "                test_loss = criterion(test_outputs, y_test_tensor).item()\n",
        "                test_mae = torch.mean(torch.abs(test_outputs - y_test_tensor)).item()\n",
        "\n",
        "            print(f\"\\nðŸ“Š Final LSTM Model Performance:\")\n",
        "            print(f\"Training Loss: {train_loss:.6f} | MAE: {train_mae:.6f}\")\n",
        "            print(f\"Validation Loss: {val_loss:.6f} | MAE: {val_mae:.6f}\")\n",
        "            print(f\"Test Loss: {test_loss:.6f} | MAE: {test_mae:.6f}\")\n",
        "\n",
        "            # Store optimization results\n",
        "            self.lstm_optimization_results = {\n",
        "                'best_params': best_params,\n",
        "                'all_results': optimization_results,\n",
        "                'final_scores': {\n",
        "                    'train': (train_loss, train_mae),\n",
        "                    'val': (val_loss, val_mae),\n",
        "                    'test': (test_loss, test_mae)\n",
        "                },\n",
        "                'training_history': training_history\n",
        "            }\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training LSTM: {e}\")\n",
        "            return False\n",
        "\n",
        "    def forecast(self, days_ahead=30):\n",
        "        \"\"\"Generate hybrid forecast using PyTorch model\"\"\"\n",
        "        try:\n",
        "            print(f\"Generating {days_ahead} day forecast...\")\n",
        "\n",
        "            # ARIMA forecast\n",
        "            arima_forecast = self.arima_model.forecast(steps=days_ahead)\n",
        "            arima_conf_int = self.arima_model.get_forecast(steps=days_ahead).conf_int()\n",
        "\n",
        "            # Prepare data for LSTM residual prediction\n",
        "            last_sequence = self.prepare_last_sequence()\n",
        "\n",
        "            # LSTM residual forecast using PyTorch\n",
        "            lstm_residual_forecast = []\n",
        "            current_sequence = last_sequence.copy()\n",
        "\n",
        "            # Convert to tensor and move to device\n",
        "            self.lstm_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for _ in range(days_ahead):\n",
        "                    # Convert to tensor\n",
        "                    sequence_tensor = torch.FloatTensor(current_sequence).unsqueeze(0).to(self.device)\n",
        "                    \n",
        "                    # Predict next residual\n",
        "                    next_residual = self.lstm_model(sequence_tensor).cpu().numpy()[0, 0]\n",
        "                    lstm_residual_forecast.append(next_residual)\n",
        "\n",
        "                    # Update sequence\n",
        "                    if self.use_cupy:\n",
        "                        current_sequence_gpu = cp.asarray(current_sequence)\n",
        "                        current_sequence_gpu = cp.roll(current_sequence_gpu, -1, axis=0)\n",
        "                        current_sequence_gpu[-1, 0] = next_residual\n",
        "                        \n",
        "                        # Update with scaled exogenous features\n",
        "                        next_date = self.data.index[-1] + timedelta(days=_ + 1)\n",
        "                        scaled_exog = self.get_scaled_exog_features(next_date)\n",
        "                        if scaled_exog is not None:\n",
        "                            current_sequence_gpu[-1, 1:] = cp.asarray(scaled_exog)\n",
        "                        \n",
        "                        current_sequence = cp.asnumpy(current_sequence_gpu)\n",
        "                    else:\n",
        "                        current_sequence = np.roll(current_sequence, -1, axis=0)\n",
        "                        current_sequence[-1, 0] = next_residual\n",
        "                        \n",
        "                        # Update with scaled exogenous features\n",
        "                        next_date = self.data.index[-1] + timedelta(days=_ + 1)\n",
        "                        scaled_exog = self.get_scaled_exog_features(next_date)\n",
        "                        if scaled_exog is not None:\n",
        "                            current_sequence[-1, 1:] = scaled_exog\n",
        "\n",
        "            # Inverse transform residuals\n",
        "            dummy_features = np.zeros((len(lstm_residual_forecast), self.scaler.scale_.shape[0]))\n",
        "            dummy_features[:, 0] = lstm_residual_forecast\n",
        "            lstm_residual_forecast = self.scaler.inverse_transform(dummy_features)[:, 0]\n",
        "\n",
        "            # Combine ARIMA + LSTM predictions\n",
        "            hybrid_forecast = arima_forecast + lstm_residual_forecast\n",
        "\n",
        "            # Get news sentiment and adjust predictions\n",
        "            sentiment_score, news_articles = self.fetch_news_sentiment()\n",
        "            sentiment_adjustment = sentiment_score * 0.05  # 5% max adjustment based on sentiment\n",
        "            adjusted_forecast = hybrid_forecast * (1 + sentiment_adjustment)\n",
        "\n",
        "            # Create forecast dates\n",
        "            last_date = self.data.index[-1]\n",
        "            forecast_dates = pd.date_range(\n",
        "                start=last_date + timedelta(days=1),\n",
        "                periods=days_ahead,\n",
        "                freq='D'\n",
        "            )\n",
        "\n",
        "            # Store predictions\n",
        "            self.predictions = pd.DataFrame({\n",
        "                'date': forecast_dates,\n",
        "                'arima_forecast': arima_forecast,\n",
        "                'hybrid_forecast': hybrid_forecast,\n",
        "                'adjusted_forecast': adjusted_forecast,\n",
        "                'lower_bound': arima_conf_int.iloc[:, 0],\n",
        "                'upper_bound': arima_conf_int.iloc[:, 1],\n",
        "                'sentiment_score': sentiment_score\n",
        "            })\n",
        "\n",
        "            return self.predictions, news_articles, sentiment_score\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating forecast: {e}\")\n",
        "            return None, [], 0\n",
        "\n",
        "    def prepare_last_sequence(self, look_back=60):\n",
        "        \"\"\"Prepare the last sequence for LSTM prediction using CuPy acceleration\"\"\"\n",
        "        try:\n",
        "            features = pd.DataFrame({\n",
        "                'residuals': self.arima_residuals,\n",
        "                'volume': self.data['Volume'][self.arima_residuals.index],\n",
        "                'rsi': self.data['RSI'][self.arima_residuals.index],\n",
        "                'volatility': self.data['Volatility'][self.arima_residuals.index]\n",
        "            }).fillna(method='ffill').dropna()\n",
        "\n",
        "            scaled_features = self.scaler.transform(features)\n",
        "            \n",
        "            if self.use_cupy:\n",
        "                # Use CuPy for faster array operations\n",
        "                scaled_features_gpu = cp.asarray(scaled_features)\n",
        "                last_sequence = cp.asnumpy(scaled_features_gpu[-look_back:])\n",
        "            else:\n",
        "                last_sequence = scaled_features[-look_back:]\n",
        "                \n",
        "            return last_sequence\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing last sequence: {e}\")\n",
        "            return np.zeros((60, 4))\n",
        "\n",
        "    def calculate_trading_signals(self, current_price):\n",
        "        \"\"\"Calculate buy/sell signals and profit projections with CuPy acceleration\"\"\"\n",
        "        try:\n",
        "            if self.predictions is None:\n",
        "                return None\n",
        "\n",
        "            forecast_prices = self.predictions['adjusted_forecast'].values\n",
        "            dates = self.predictions['date'].values\n",
        "\n",
        "            # Use CuPy for faster calculations if available\n",
        "            if self.use_cupy:\n",
        "                forecast_gpu = cp.asarray(forecast_prices)\n",
        "                max_price = float(cp.max(forecast_gpu))\n",
        "                min_price = float(cp.min(forecast_gpu))\n",
        "                max_idx = int(cp.argmax(forecast_gpu))\n",
        "                min_idx = int(cp.argmin(forecast_gpu))\n",
        "            else:\n",
        "                max_price = forecast_prices.max()\n",
        "                min_price = forecast_prices.min()\n",
        "                max_idx = np.argmax(forecast_prices)\n",
        "                min_idx = np.argmin(forecast_prices)\n",
        "\n",
        "            # Trading recommendations\n",
        "            if current_price < forecast_prices[0]:\n",
        "                recommendation = \"BUY\"\n",
        "                target_price = max_price\n",
        "                potential_profit = ((target_price - current_price) / current_price) * 100\n",
        "                optimal_sell_date = dates[max_idx]\n",
        "            elif current_price > forecast_prices[-1]:\n",
        "                recommendation = \"SELL\"\n",
        "                target_price = min_price\n",
        "                potential_profit = ((current_price - target_price) / current_price) * 100\n",
        "                optimal_sell_date = dates[min_idx]\n",
        "            else:\n",
        "                recommendation = \"HOLD\"\n",
        "                target_price = forecast_prices[-1]\n",
        "                potential_profit = ((target_price - current_price) / current_price) * 100\n",
        "                optimal_sell_date = dates[-1]\n",
        "\n",
        "            return {\n",
        "                'recommendation': recommendation,\n",
        "                'current_price': current_price,\n",
        "                'target_price': target_price,\n",
        "                'potential_profit_pct': potential_profit,\n",
        "                'optimal_sell_date': optimal_sell_date,\n",
        "                'forecast_prices': forecast_prices,\n",
        "                'dates': dates\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating trading signals: {e}\")\n",
        "            return None\n",
        "\n",
        "    def evaluate_model_performance(self):\n",
        "        \"\"\"Comprehensive model evaluation and comparison with CuPy acceleration\"\"\"\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ðŸ“Š COMPREHENSIVE MODEL EVALUATION\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            # Get recent data for backtesting\n",
        "            test_data = self.data['Close'].tail(60).values  # Last 60 days for testing\n",
        "            train_data = self.data['Close'][:-60].values\n",
        "\n",
        "            # 1. Baseline Models\n",
        "            naive_forecast = [train_data[-1]] * len(test_data)  # Naive: last value\n",
        "            sma_forecast = [np.mean(train_data[-20:])] * len(test_data)  # Simple moving average\n",
        "\n",
        "            # 2. ARIMA-only predictions\n",
        "            arima_only_model = ARIMA(train_data, order=self.arima_model.model.order).fit()\n",
        "            arima_forecast = arima_only_model.forecast(steps=len(test_data))\n",
        "\n",
        "            # 3. Hybrid model predictions (ARIMA + LSTM)\n",
        "            hybrid_predictions = self.generate_backtest_predictions(len(test_data))\n",
        "\n",
        "            # Calculate metrics using CuPy if available\n",
        "            def calculate_metrics(actual, predicted):\n",
        "                if self.use_cupy:\n",
        "                    actual_gpu = cp.asarray(actual)\n",
        "                    predicted_gpu = cp.asarray(predicted)\n",
        "                    \n",
        "                    mse = float(cp.mean((actual_gpu - predicted_gpu) ** 2))\n",
        "                    rmse = float(cp.sqrt(mse))\n",
        "                    mae = float(cp.mean(cp.abs(actual_gpu - predicted_gpu)))\n",
        "                    mape = float(cp.mean(cp.abs((actual_gpu - predicted_gpu) / actual_gpu)) * 100)\n",
        "\n",
        "                    # Directional accuracy\n",
        "                    actual_direction = cp.sign(cp.diff(actual_gpu))\n",
        "                    pred_direction = cp.sign(cp.diff(predicted_gpu))\n",
        "                    directional_accuracy = float(cp.mean(actual_direction == pred_direction) * 100)\n",
        "                else:\n",
        "                    mse = np.mean((actual - predicted) ** 2)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    mae = np.mean(np.abs(actual - predicted))\n",
        "                    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
        "\n",
        "                    # Directional accuracy\n",
        "                    actual_direction = np.sign(np.diff(actual))\n",
        "                    pred_direction = np.sign(np.diff(predicted))\n",
        "                    directional_accuracy = np.mean(actual_direction == pred_direction) * 100\n",
        "\n",
        "                return {\n",
        "                    'MSE': mse,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAE': mae,\n",
        "                    'MAPE': mape,\n",
        "                    'Directional_Accuracy': directional_accuracy\n",
        "                }\n",
        "\n",
        "            # Evaluate all models\n",
        "            models_performance = {\n",
        "                'Naive': calculate_metrics(test_data, naive_forecast),\n",
        "                'Moving_Average': calculate_metrics(test_data, sma_forecast),\n",
        "                'ARIMA_Only': calculate_metrics(test_data, arima_forecast),\n",
        "                'Hybrid_ARIMA_LSTM': calculate_metrics(test_data, hybrid_predictions)\n",
        "            }\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\nðŸ“ˆ Model Comparison Results:\")\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"{'Model':<20} {'RMSE':<10} {'MAE':<10} {'MAPE':<10} {'Dir_Acc':<10}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            best_model = None\n",
        "            best_rmse = float('inf')\n",
        "\n",
        "            for model_name, metrics in models_performance.items():\n",
        "                print(f\"{model_name:<20} {metrics['RMSE']:<10.4f} {metrics['MAE']:<10.4f} \"\n",
        "                      f\"{metrics['MAPE']:<10.2f}% {metrics['Directional_Accuracy']:<10.1f}%\")\n",
        "\n",
        "                if metrics['RMSE'] < best_rmse:\n",
        "                    best_rmse = metrics['RMSE']\n",
        "                    best_model = model_name\n",
        "\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"ðŸ† Best performing model: {best_model}\")\n",
        "\n",
        "            # Statistical significance test\n",
        "            self.statistical_significance_test(test_data, arima_forecast, hybrid_predictions)\n",
        "\n",
        "            # Store evaluation results\n",
        "            self.model_evaluation = {\n",
        "                'performance_metrics': models_performance,\n",
        "                'best_model': best_model,\n",
        "                'test_period_length': len(test_data)\n",
        "            }\n",
        "\n",
        "            return models_performance\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in model evaluation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def statistical_significance_test(self, actual, arima_pred, hybrid_pred):\n",
        "        \"\"\"Perform statistical significance test between models with CuPy acceleration\"\"\"\n",
        "        try:\n",
        "            from scipy.stats import ttest_rel\n",
        "\n",
        "            if self.use_cupy:\n",
        "                actual_gpu = cp.asarray(actual)\n",
        "                arima_pred_gpu = cp.asarray(arima_pred)\n",
        "                hybrid_pred_gpu = cp.asarray(hybrid_pred)\n",
        "                \n",
        "                arima_errors = cp.asnumpy(cp.abs(actual_gpu - arima_pred_gpu))\n",
        "                hybrid_errors = cp.asnumpy(cp.abs(actual_gpu - hybrid_pred_gpu))\n",
        "            else:\n",
        "                arima_errors = np.abs(actual - arima_pred)\n",
        "                hybrid_errors = np.abs(actual - hybrid_pred)\n",
        "\n",
        "            # Paired t-test\n",
        "            t_stat, p_value = ttest_rel(arima_errors, hybrid_errors)\n",
        "\n",
        "            print(f\"\\nðŸ”¬ Statistical Significance Test (Paired t-test):\")\n",
        "            print(f\"H0: No difference between ARIMA and Hybrid model errors\")\n",
        "            print(f\"t-statistic: {t_stat:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                winner = \"Hybrid\" if np.mean(hybrid_errors) < np.mean(arima_errors) else \"ARIMA\"\n",
        "                print(f\"âœ… Significant difference detected (p < 0.05). {winner} model is significantly better.\")\n",
        "            else:\n",
        "                print(\"âŒ No significant difference between models (p >= 0.05)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in significance test: {e}\")\n",
        "\n",
        "    def generate_backtest_predictions(self, n_steps):\n",
        "        \"\"\"Generate predictions for backtesting using PyTorch model\"\"\"\n",
        "        try:\n",
        "            # Get ARIMA predictions\n",
        "            arima_pred = self.arima_model.forecast(steps=n_steps)\n",
        "\n",
        "            # Simulate LSTM residual predictions using the trained PyTorch model\n",
        "            recent_residuals = self.arima_residuals.tail(20).values\n",
        "            \n",
        "            if self.use_cupy:\n",
        "                recent_residuals_gpu = cp.asarray(recent_residuals)\n",
        "                residual_pattern = float(cp.mean(recent_residuals_gpu))\n",
        "            else:\n",
        "                residual_pattern = np.mean(recent_residuals)\n",
        "\n",
        "            # Simple residual forecast (this would be replaced by actual LSTM predictions)\n",
        "            lstm_residuals = [residual_pattern * (0.9 ** i) for i in range(n_steps)]\n",
        "\n",
        "            # Combine predictions\n",
        "            if self.use_cupy:\n",
        "                arima_pred_gpu = cp.asarray(arima_pred)\n",
        "                lstm_residuals_gpu = cp.asarray(lstm_residuals)\n",
        "                hybrid_pred = cp.asnumpy(arima_pred_gpu + lstm_residuals_gpu)\n",
        "            else:\n",
        "                hybrid_pred = arima_pred + lstm_residuals\n",
        "\n",
        "            return hybrid_pred\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating backtest predictions: {e}\")\n",
        "            return np.zeros(n_steps)\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        \"\"\"Save the trained PyTorch model and associated data\"\"\"\n",
        "        try:\n",
        "            model_data = {\n",
        "                'symbol': self.symbol,\n",
        "                'arima_model': self.arima_model,\n",
        "                'lstm_model_state_dict': self.lstm_model.state_dict() if self.lstm_model else None,\n",
        "                'lstm_model_params': {\n",
        "                    'input_size': 4,  # Adjust based on your features\n",
        "                    'lstm_units': getattr(self, 'lstm_optimization_results', {}).get('best_params', {}).get('lstm_units', 50),\n",
        "                    'num_layers': getattr(self, 'lstm_optimization_results', {}).get('best_params', {}).get('num_layers', 3),\n",
        "                    'dropout_rate': getattr(self, 'lstm_optimization_results', {}).get('best_params', {}).get('dropout_rate', 0.2)\n",
        "                },\n",
        "                'scaler': self.scaler,\n",
        "                'last_date': self.data.index[-1],\n",
        "                'last_price': self.data['Close'].iloc[-1],\n",
        "                'predictions': self.predictions,\n",
        "                'data_tail': self.data.tail(100),  # Keep last 100 days for reference\n",
        "                'arima_optimization_results': getattr(self, 'arima_optimization_results', None),\n",
        "                'lstm_optimization_results': getattr(self, 'lstm_optimization_results', None),\n",
        "                'model_evaluation': getattr(self, 'model_evaluation', None),\n",
        "                'device': str(self.device),\n",
        "                'use_cupy': self.use_cupy\n",
        "            }\n",
        "\n",
        "            with open(filename, 'wb') as f:\n",
        "                pickle.dump(model_data, f)\n",
        "\n",
        "            print(f\"Model saved to {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        \"\"\"Load a saved PyTorch model\"\"\"\n",
        "        try:\n",
        "            with open(filename, 'rb') as f:\n",
        "                model_data = pickle.load(f)\n",
        "\n",
        "            self.symbol = model_data['symbol']\n",
        "            self.arima_model = model_data['arima_model']\n",
        "            self.scaler = model_data['scaler']\n",
        "            self.predictions = model_data['predictions']\n",
        "            self.data = model_data['data_tail']\n",
        "\n",
        "            # Recreate LSTM model\n",
        "            if model_data['lstm_model_state_dict'] is not None:\n",
        "                params = model_data['lstm_model_params']\n",
        "                self.lstm_model = LSTMModel(\n",
        "                    input_size=params['input_size'],\n",
        "                    lstm_units=params['lstm_units'],\n",
        "                    num_layers=params['num_layers'],\n",
        "                    dropout_rate=params['dropout_rate']\n",
        "                ).to(self.device)\n",
        "                self.lstm_model.load_state_dict(model_data['lstm_model_state_dict'])\n",
        "\n",
        "            print(f\"Model loaded from {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_scaled_exog_features(self, date):\n",
        "        \"\"\"\n",
        "        Fetch or compute exogenous features for the given date and scale them.\n",
        "        This example assumes exogenous features include calendar-based features\n",
        "        (computable for future dates) and potentially real-time fetchable features\n",
        "        like stock volume or indicators if the date is in the past or current.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Define computable calendar features\n",
        "            weekday = date.weekday()  # 0-6\n",
        "            month = date.month  # 1-12\n",
        "            quarter = (date.month - 1) // 3 + 1\n",
        "\n",
        "            # Placeholder for fetchable features, e.g., volume\n",
        "            volume = np.nan  # Default to NaN for future dates\n",
        "\n",
        "            current_date = datetime.now().date()  # Get current date for comparison\n",
        "            if date.date() <= current_date:\n",
        "                # Attempt to fetch actual data if date is not future\n",
        "                try:\n",
        "                    stock_data = yf.download(self.symbol, start=date, end=date + pd.Timedelta(days=1))\n",
        "                    if not stock_data.empty:\n",
        "                        volume = stock_data['Volume'].iloc[0]  # Get volume if available\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Handle NaN for volume (e.g., use mean from historical data)\n",
        "            if np.isnan(volume):\n",
        "                volume = self.data['Volume'].mean() if 'Volume' in self.data.columns else 0  # Fallback\n",
        "\n",
        "            # Collect features (must match training: order and count)\n",
        "            # Example: assume 3 exog features: volume, rsi, volatility (excluding residuals)\n",
        "            # Simplified - in practice you'd compute actual RSI and volatility\n",
        "            rsi = 50.0  # Default RSI\n",
        "            volatility = self.data['Volatility'].mean() if 'Volatility' in self.data.columns else 0.02\n",
        "\n",
        "            features = [volume, rsi, volatility]\n",
        "\n",
        "            # Number of exog features (total scaled features - 1 for residual)\n",
        "            num_exog = self.scaler.n_features_in_ - 1\n",
        "\n",
        "            if len(features) != num_exog:\n",
        "                # Pad or trim features to match expected size\n",
        "                if len(features) < num_exog:\n",
        "                    features.extend([0.0] * (num_exog - len(features)))\n",
        "                else:\n",
        "                    features = features[:num_exog]\n",
        "\n",
        "            # Create dummy array for scaling (residual=0, then exog)\n",
        "            dummy = np.zeros(self.scaler.n_features_in_)\n",
        "            dummy[1:] = features\n",
        "\n",
        "            # Scale and return only the scaled exog part\n",
        "            scaled_full = self.scaler.transform(dummy.reshape(1, -1))[0]\n",
        "            scaled_exog = scaled_full[1:]\n",
        "\n",
        "            return scaled_exog\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching/computing features for {date}: {e}\")\n",
        "            # Fallback: return zeros or last known scaled exog\n",
        "            return np.zeros(self.scaler.n_features_in_ - 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_5PJF3eaDvG",
        "outputId": "8d7a1803-e11a-4401-c300-a33ed7a3a245"
      },
      "outputs": [],
      "source": [
        "def train_and_save_model_pytorch(symbol, filename):\n",
        "    \"\"\"Complete training pipeline using PyTorch\"\"\"\n",
        "    print(f\"Starting PyTorch training for {symbol}...\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"CuPy available: {cp.cuda.is_available()}\")\n",
        "\n",
        "    forecaster = StockForecasterPyTorch(symbol)\n",
        "\n",
        "    # Step 1: Fetch data\n",
        "    if not forecaster.fetch_stock_data():\n",
        "        return False\n",
        "\n",
        "    # Step 2: Fit ARIMA\n",
        "    if not forecaster.fit_arima():\n",
        "        return False\n",
        "\n",
        "    # Step 3: Train LSTM\n",
        "    if not forecaster.train_lstm():\n",
        "        return False\n",
        "\n",
        "    # Step 4: Generate forecast\n",
        "    predictions, news, sentiment = forecaster.forecast(30)\n",
        "    if predictions is None:\n",
        "        return False\n",
        "\n",
        "    # Step 5: Evaluate model performance\n",
        "    forecaster.evaluate_model_performance()\n",
        "\n",
        "    # Step 6: Save model\n",
        "    if not forecaster.save_model(filename):\n",
        "        return False\n",
        "\n",
        "    # Display results\n",
        "    current_price = forecaster.data['Close'].iloc[-1]\n",
        "    signals = forecaster.calculate_trading_signals(current_price)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"PYTORCH TRAINING COMPLETED FOR {symbol}\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Current Price: ${current_price:.2f}\")\n",
        "    print(f\"Sentiment Score: {sentiment:.3f}\")\n",
        "    print(f\"Recommendation: {signals['recommendation']}\")\n",
        "    print(f\"Target Price: ${signals['target_price']:.2f}\")\n",
        "    print(f\"Potential Profit: {signals['potential_profit_pct']:.2f}%\")\n",
        "    print(f\"Model saved to: {filename}\")\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Historical prices\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(forecaster.data.index[-100:], forecaster.data['Close'][-100:], label='Historical')\n",
        "    plt.plot(predictions['date'], predictions['adjusted_forecast'], 'r--', label='Forecast')\n",
        "    plt.fill_between(predictions['date'],\n",
        "                     predictions['lower_bound'],\n",
        "                     predictions['upper_bound'],\n",
        "                     alpha=0.3)\n",
        "    plt.title(f'{symbol} - Price Forecast (PyTorch)')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # ARIMA residuals\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(forecaster.arima_residuals)\n",
        "    plt.title('ARIMA Residuals')\n",
        "\n",
        "    # Technical indicators\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(forecaster.data.index[-100:], forecaster.data['RSI'][-100:])\n",
        "    plt.title('RSI')\n",
        "    plt.axhline(y=70, color='r', linestyle='--')\n",
        "    plt.axhline(y=30, color='g', linestyle='--')\n",
        "\n",
        "    # Volume\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.bar(forecaster.data.index[-20:], forecaster.data['Volume'][-20:])\n",
        "    plt.title('Volume (Last 20 days)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{symbol}_pytorch_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_b0VeXKaALf"
      },
      "outputs": [],
      "source": [
        "# ARIMA\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from pmdarima import auto_arima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TQGqWu5ax13",
        "outputId": "d9db22f8-7d3a-4e89-f19a-09dacff1354d"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Replace with your stock symbol and desired filename\n",
        "    STOCK_SYMBOL = \"AAPL\"  # Change this to any stock symbol\n",
        "    MODEL_FILENAME = f\"{STOCK_SYMBOL}_pytorch_forecaster.pkl\"\n",
        "\n",
        "    # Train and save model\n",
        "    success = train_and_save_model_pytorch(STOCK_SYMBOL, MODEL_FILENAME)\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nâœ… PyTorch training completed successfully!\")\n",
        "        print(f\"ðŸ“„ Download the file '{MODEL_FILENAME}' to use in your application\")\n",
        "        \n",
        "        # Display GPU/CuPy status\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"ðŸš€ GPU acceleration used: {torch.cuda.get_device_name()}\")\n",
        "        if cp.cuda.is_available():\n",
        "            print(f\"âš¡ CuPy acceleration enabled\")\n",
        "    else:\n",
        "        print(\"\\nâŒ Training failed. Please check the error messages above.\")\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
