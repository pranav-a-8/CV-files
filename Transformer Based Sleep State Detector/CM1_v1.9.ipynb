{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-31T16:17:43.563493Z",
     "iopub.status.busy": "2025-07-31T16:17:43.562963Z",
     "iopub.status.idle": "2025-07-31T16:17:56.053668Z",
     "shell.execute_reply": "2025-07-31T16:17:56.052983Z",
     "shell.execute_reply.started": "2025-07-31T16:17:43.563443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Configurations (GPU Optimized)\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from scipy.signal import firwin\n",
    "from scipy.fft import fft\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import glob\n",
    "\n",
    "# GPU Optimization Settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False  # For better performance\n",
    "\n",
    "# Check GPU capability and enable features accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_capability = torch.cuda.get_device_capability(0)\n",
    "    gpu_capability_major = gpu_capability[0]\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Capability: {gpu_capability[0]}.{gpu_capability[1]}\")\n",
    "    \n",
    "    # Enable TF32 only on Ampere and newer (capability >= 8.0)\n",
    "    if gpu_capability_major >= 8:\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        print(\"✓ TF32 enabled for Ampere+ GPU\")\n",
    "    else:\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        print(\"✓ TF32 disabled for older GPU\")\n",
    "    \n",
    "    # Check if torch.compile is supported (capability >= 7.0)\n",
    "    COMPILE_SUPPORTED = gpu_capability_major >= 7\n",
    "    if not COMPILE_SUPPORTED:\n",
    "        print(\"⚠ torch.compile not supported on this GPU (capability < 7.0)\")\n",
    "        # Suppress torch.compile errors\n",
    "        import torch._dynamo\n",
    "        torch._dynamo.config.suppress_errors = True\n",
    "else:\n",
    "    COMPILE_SUPPORTED = False\n",
    "\n",
    "# The defined class from previous\n",
    "class CompetitionMetric:\n",
    "    def __init__(self):\n",
    "        self.bfrb_gestures = [\n",
    "            'Forehead - pull hairline',\n",
    "            'Neck - pinch skin',\n",
    "            'Forehead - scratch',\n",
    "            'Eyelash - pull hair',\n",
    "            'Eyebrow - pull hair',\n",
    "            'Neck - scratch',\n",
    "            'Above ear - pull hair',\n",
    "            'Cheek - pinch skin',\n",
    "        ]\n",
    "\n",
    "    def calculate_hierarchical_f1(self, true_df, pred_df):\n",
    "        from sklearn.metrics import f1_score\n",
    "        y_true = true_df['gesture'].values\n",
    "        y_pred = pred_df['gesture'].values\n",
    "\n",
    "        # Level 1: BFRB vs non-BFRB (binary)\n",
    "        y_true_level1 = [1 if g in self.bfrb_gestures else 0 for g in y_true]\n",
    "        y_pred_level1 = [1 if g in self.bfrb_gestures else 0 for g in y_pred]\n",
    "        f1_level1 = f1_score(y_true_level1, y_pred_level1, average='binary')\n",
    "\n",
    "        # Level 2: Specific gestures (macro F1 on all classes)\n",
    "        f1_level2 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        # Hierarchical F1: average of levels\n",
    "        return (f1_level1 + f1_level2) / 2\n",
    "\n",
    "# Configuration\n",
    "TRAIN = True  # Set to False for inference only\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/cmi3-models-p\")  # Used when TRAIN=False\n",
    "EXPORT_DIR = Path(\"./\")  # Artefacts saved here\n",
    "BATCH_SIZE = 512  # Increased for better GPU utilization\n",
    "PAD_PERCENTILE = 100\n",
    "maxlen = PAD_PERCENTILE\n",
    "LR_INIT = 1e-3\n",
    "WD = 3e-3\n",
    "PATIENCE = 40\n",
    "FOLDS = 5\n",
    "random_state = 42\n",
    "epochs_warmup = 20\n",
    "warmup_lr_init = 1.822126131809773e-05\n",
    "lr_min = 3.810323058740104e-09\n",
    "USE_AMP = True\n",
    "ACCUM_STEPS = 1  # Reduced since we increased batch size\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"▶ Imports ready · PyTorch {torch.__version__} · Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"▶ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "\n",
    "# Global mean/std for normalization - Move to GPU immediately\n",
    "fft_bins = 10\n",
    "extra_channels = 6 * fft_bins\n",
    "base_mean = torch.tensor([\n",
    "    0, 0, 0, 0, 0, 0, 9.0319e-03, 1.0849e+00, -2.6186e-03, 3.7651e-03,\n",
    "    -5.3660e-03, -2.8177e-03, 1.3318e-03, -1.5876e-04, 6.3495e-01,\n",
    "    6.2877e-01, 6.0607e-01, 6.2142e-01, 6.3808e-01, 6.5420e-01,\n",
    "    7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02, 2.9704e-02,\n",
    "    -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02,\n",
    "    1.5799e-02, 1.0016e-02\n",
    "], dtype=torch.float32, device=device).view(1, -1, 1)\n",
    "\n",
    "base_std = torch.tensor([\n",
    "    1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162,\n",
    "    0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973,\n",
    "    1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941,\n",
    "    0.3421, 0.8156, 0.6565, 1.1034, 1.5577\n",
    "], dtype=torch.float32, device=device).view(1, -1, 1) + 1e-8\n",
    "\n",
    "# Extend for FFT channels\n",
    "mean = torch.cat([base_mean, torch.zeros(1, extra_channels, 1, device=device)], dim=1)\n",
    "std = torch.cat([base_std, torch.ones(1, extra_channels, 1, device=device)], dim=1)\n",
    "\n",
    "print(\"✓ GPU tensors initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T16:17:56.055484Z",
     "iopub.status.busy": "2025-07-31T16:17:56.055099Z",
     "iopub.status.idle": "2025-07-31T16:17:56.092153Z",
     "shell.execute_reply": "2025-07-31T16:17:56.091354Z",
     "shell.execute_reply.started": "2025-07-31T16:17:56.055466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Define Model Classes and Augmentation (GPU Optimized)\n",
    "\n",
    "class ImuFeatureExtractor(nn.Module):\n",
    "    def __init__(self, fs=100., add_quaternion=False, fft_bins=10):\n",
    "        super().__init__()\n",
    "        self.fs = fs\n",
    "        self.add_quaternion = add_quaternion\n",
    "        self.fft_bins = fft_bins\n",
    "\n",
    "        k = 15\n",
    "        # Pre-computed filter coefficients moved to GPU\n",
    "        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2, groups=6, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.lpf_acc = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        \n",
    "        # Pre-allocate FFT workspace\n",
    "        self.register_buffer('fft_workspace', torch.zeros(1, 6, fft_bins))\n",
    "\n",
    "    def forward(self, imu):\n",
    "        B, C, T = imu.shape\n",
    "        acc = imu[:, 0:3, :]\n",
    "        gyro = imu[:, 3:6, :]\n",
    "\n",
    "        # Basic features (vectorized operations)\n",
    "        acc_mag = torch.norm(acc, dim=1, keepdim=True)\n",
    "        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n",
    "        \n",
    "        # Use diff for jerk calculation (more efficient)\n",
    "        jerk = F.pad(torch.diff(acc, dim=-1), (1, 0))\n",
    "        gyro_delta = F.pad(torch.diff(gyro, dim=-1), (1, 0))\n",
    "        \n",
    "        acc_pow = acc.pow(2)\n",
    "        gyro_pow = gyro.pow(2)\n",
    "        \n",
    "        acc_lpf = self.lpf_acc(acc)\n",
    "        acc_hpf = acc - acc_lpf\n",
    "        gyro_lpf = self.lpf_gyro(gyro)\n",
    "        gyro_hpf = gyro - gyro_lpf\n",
    "\n",
    "        # Optimized FFT features\n",
    "        acc_fft = torch.abs(torch.fft.fft(acc, dim=-1))[:, :, :self.fft_bins]\n",
    "        gyro_fft = torch.abs(torch.fft.fft(gyro, dim=-1))[:, :, :self.fft_bins]\n",
    "\n",
    "        # More efficient reshaping and expansion\n",
    "        acc_fft = acc_fft.flatten(1, 2).unsqueeze(-1).expand(-1, -1, T)\n",
    "        gyro_fft = gyro_fft.flatten(1, 2).unsqueeze(-1).expand(-1, -1, T)\n",
    "\n",
    "        features = [\n",
    "            acc, gyro, acc_mag, gyro_mag, jerk, gyro_delta,\n",
    "            acc_pow, gyro_pow, acc_lpf, acc_hpf, gyro_lpf, gyro_hpf,\n",
    "            acc_fft, gyro_fft\n",
    "        ]\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = torch.tanh(self.attention(x))\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)\n",
    "        return context\n",
    "\n",
    "class TwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, \n",
    "                 dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], \n",
    "                 feature_engineering=True, fft_bins=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        \n",
    "        if feature_engineering:\n",
    "            self.imu_fe = ImuFeatureExtractor(fft_bins=fft_bins, **kwargs)\n",
    "            imu_dim = 32 + 6 * fft_bins\n",
    "        else:\n",
    "            self.imu_fe = nn.Identity()\n",
    "            imu_dim = imu_dim_raw\n",
    "\n",
    "        self.imu_dim = imu_dim\n",
    "        self.tof_dim = tof_dim\n",
    "        self.fir_nchan = 7\n",
    "\n",
    "        # FIR filter coefficients as buffer (moved to GPU automatically)\n",
    "        numtaps = 33\n",
    "        fir_coef = firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False)\n",
    "        fir_kernel = torch.tensor(fir_coef, dtype=torch.float32).view(1, 1, -1)\n",
    "        fir_kernel = fir_kernel.repeat(7, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "\n",
    "        # Branches with optimized dimensions\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1 = nn.BatchNorm1d(64)\n",
    "        self.tof_pool1 = nn.MaxPool1d(2)\n",
    "        self.tof_drop1 = nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2 = nn.BatchNorm1d(128)\n",
    "        self.tof_pool2 = nn.MaxPool1d(2)\n",
    "        self.tof_drop2 = nn.Dropout(dropouts[3])\n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256, nhead=8, dim_feedforward=512, \n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # Attention and classification\n",
    "        self.attention = AttentionLayer(256)\n",
    "        self.dense1 = nn.Linear(256, 256, bias=False)\n",
    "        self.bn_dense1 = nn.BatchNorm1d(256)\n",
    "        self.drop1 = nn.Dropout(dropouts[5])\n",
    "        self.dense2 = nn.Linear(256, 128, bias=False)\n",
    "        self.bn_dense2 = nn.BatchNorm1d(128)\n",
    "        self.drop2 = nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Efficient tensor operations\n",
    "        imu = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "\n",
    "        # Feature extraction\n",
    "        imu = self.imu_fe(imu)\n",
    "        \n",
    "        # Apply FIR filter\n",
    "        filtered = F.conv1d(\n",
    "            imu[:, :self.fir_nchan, :],\n",
    "            self.fir_kernel,\n",
    "            padding=self.fir_kernel.shape[-1] // 2,\n",
    "            groups=self.fir_nchan,\n",
    "        )\n",
    "        imu = torch.cat([filtered, imu[:, self.fir_nchan:, :]], dim=1)\n",
    "        \n",
    "        # Normalization (already on GPU)\n",
    "        imu = (imu - mean) / std\n",
    "\n",
    "        # Process branches\n",
    "        x1 = self.imu_block1(imu)\n",
    "        x1 = self.imu_block2(x1)\n",
    "        \n",
    "        x2 = F.relu(self.tof_bn1(self.tof_conv1(tof)))\n",
    "        x2 = self.tof_drop1(self.tof_pool1(x2))\n",
    "        x2 = F.relu(self.tof_bn2(self.tof_conv2(x2)))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(x2))\n",
    "\n",
    "        # Merge and process with RNN/Transformer\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        \n",
    "        lstm_out, _ = self.bilstm(merged)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        \n",
    "        # Transformer\n",
    "        trans_out = self.transformer_encoder(lstm_out)\n",
    "        \n",
    "        # Attention and classification\n",
    "        attended = self.attention(trans_out)\n",
    "        \n",
    "        x = F.relu(self.bn_dense1(self.dense1(attended)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn_dense2(self.dense2(x)))\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# GPU-optimized augmentation\n",
    "class Augment:\n",
    "    def __init__(self, p_jitter=0.8, sigma=0.02, scale_range=[0.9,1.1],\n",
    "                 p_dropout=0.3, p_moda=0.5, drift_std=0.005, drift_max=0.25,\n",
    "                 p_time_warp=0.3, warp_factor=0.1, p_freq_noise=0.3, freq_sigma=0.01):\n",
    "        self.p_jitter = p_jitter\n",
    "        self.sigma = sigma\n",
    "        self.scale_min, self.scale_max = scale_range\n",
    "        self.p_dropout = p_dropout\n",
    "        self.p_moda = p_moda\n",
    "        self.drift_std = drift_std\n",
    "        self.drift_max = drift_max\n",
    "        self.p_time_warp = p_time_warp\n",
    "        self.warp_factor = warp_factor\n",
    "        self.p_freq_noise = p_freq_noise\n",
    "        self.freq_sigma = freq_sigma\n",
    "\n",
    "    def time_warp(self, x):\n",
    "        t = np.arange(x.shape[0])\n",
    "        warp = np.random.normal(1, self.warp_factor, size=len(t))\n",
    "        warp = np.cumsum(warp)\n",
    "        warp = (warp - warp.min()) / (warp.max() - warp.min()) * (len(t) - 1)\n",
    "        return np.interp(t, warp, x)\n",
    "\n",
    "    def freq_noise(self, x):\n",
    "        freq = fft(x, axis=0)\n",
    "        noise = np.random.normal(0, self.freq_sigma, freq.shape)\n",
    "        freq += noise\n",
    "        return np.real(np.fft.ifft(freq, axis=0))\n",
    "\n",
    "    def jitter_scale(self, x: np.ndarray) -> np.ndarray:\n",
    "        noise = np.random.randn(*x.shape) * self.sigma\n",
    "        scale = np.random.uniform(self.scale_min, self.scale_max, size=(1, x.shape[1]))\n",
    "        return (x + noise) * scale\n",
    "\n",
    "    def sensor_dropout(self, x: np.ndarray, imu_dim: int) -> np.ndarray:\n",
    "        if random.random() < self.p_dropout:\n",
    "            x[:, imu_dim:] = 0.0\n",
    "        return x\n",
    "\n",
    "    def motion_drift(self, x: np.ndarray, imu_dim: int) -> np.ndarray:\n",
    "        T = x.shape[0]\n",
    "        drift = np.cumsum(np.random.normal(scale=self.drift_std, size=(T, 1)), axis=0)\n",
    "        drift = np.clip(drift, -self.drift_max, self.drift_max)\n",
    "        x[:, :6] += drift\n",
    "        if imu_dim > 6:\n",
    "            x[:, 6:imu_dim] += drift\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x: np.ndarray, imu_dim: int) -> np.ndarray:\n",
    "        if random.random() < self.p_jitter:\n",
    "            x = self.jitter_scale(x)\n",
    "        if random.random() < self.p_moda:\n",
    "            x = self.motion_drift(x, imu_dim)\n",
    "        if random.random() < self.p_time_warp:\n",
    "            for col in range(x.shape[1]):\n",
    "                x[:, col] = self.time_warp(x[:, col])\n",
    "        if random.random() < self.p_freq_noise:\n",
    "            for col in range(x.shape[1]):\n",
    "                x[:, col] = self.freq_noise(x[:, col])\n",
    "        x = self.sensor_dropout(x, imu_dim)\n",
    "        return x\n",
    "\n",
    "# Utility classes\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T16:17:56.093010Z",
     "iopub.status.busy": "2025-07-31T16:17:56.092780Z",
     "iopub.status.idle": "2025-07-31T16:17:56.115666Z",
     "shell.execute_reply": "2025-07-31T16:17:56.114942Z",
     "shell.execute_reply.started": "2025-07-31T16:17:56.092985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Data Handling Functions (GPU Optimized)\n",
    "\n",
    "def pad_sequences_torch(sequences, maxlen, padding='post', truncating='post', value=0.0):\n",
    "    \"\"\"Optimized padding function\"\"\"\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - len(seq)\n",
    "            if padding == 'post':\n",
    "                seq = np.concatenate([seq, np.full((pad_len, seq.shape[1]), value)])\n",
    "            else:\n",
    "                seq = np.concatenate([np.full((pad_len, seq.shape[1]), value), seq])\n",
    "        result.append(seq)\n",
    "    return np.array(result, dtype=np.float32)\n",
    "\n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list, scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "class CMI3Dataset(Dataset):\n",
    "    def __init__(self, X_list, y_list, maxlen, mode=\"train\", imu_dim=7, augment=None):\n",
    "        self.X_list = X_list\n",
    "        self.mode = mode\n",
    "        self.y_list = y_list\n",
    "        self.maxlen = maxlen\n",
    "        self.imu_dim = imu_dim\n",
    "        self.augment = augment\n",
    "\n",
    "    def pad_sequences_torch(self, seq, maxlen, padding='post', truncating='post', value=0.0):\n",
    "        if seq.shape[0] >= maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - seq.shape[0]\n",
    "            if padding == 'post':\n",
    "                seq = np.concatenate([seq, np.full((pad_len, seq.shape[1]), value)])\n",
    "            else:\n",
    "                seq = np.concatenate([np.full((pad_len, seq.shape[1]), value), seq])\n",
    "        return seq\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X_list[index].copy()  # Avoid modifying original data\n",
    "        y = self.y_list[index]\n",
    "        \n",
    "        if self.mode == \"train\" and self.augment is not None:\n",
    "            X = self.augment(X, self.imu_dim)\n",
    "        \n",
    "        X = self.pad_sequences_torch(X, self.maxlen, 'pre', 'pre')\n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "\n",
    "# Feature engineering functions (unchanged but mentioned for completeness)\n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0\n",
    "            \n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T16:17:56.117737Z",
     "iopub.status.busy": "2025-07-31T16:17:56.117256Z",
     "iopub.status.idle": "2025-07-31T16:18:54.440144Z",
     "shell.execute_reply": "2025-07-31T16:18:54.439525Z",
     "shell.execute_reply.started": "2025-07-31T16:17:56.117718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load Dataset if TRAIN (GPU Optimized)\n",
    "if TRAIN:\n",
    "    print(\"TRAIN MODE – loading dataset …\")\n",
    "    df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "    print(f\"✓ Loaded {len(df)} samples with {len(le.classes_)} classes\")\n",
    "\n",
    "    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                 'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "    feature_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "    imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "    tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "    \n",
    "    print(f\"✓ IMU features: {len(imu_cols)}, TOF features: {len(tof_cols)}\")\n",
    "\n",
    "    # Fit scaler and save\n",
    "    scaler = StandardScaler().fit(df[feature_cols].ffill().bfill().fillna(0).values)\n",
    "    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "\n",
    "    # Process sequences\n",
    "    seq_gp = df.groupby('sequence_id')\n",
    "    X_list, y_list, id_list = [], [], []\n",
    "    \n",
    "    print(\"Processing sequences...\")\n",
    "    for seq_id, seq in tqdm(seq_gp, desc=\"Processing sequences\"):\n",
    "        mat = preprocess_sequence(seq, feature_cols, scaler)\n",
    "        X_list.append(mat)\n",
    "        y_list.append(seq['gesture_int'].iloc[0])\n",
    "        id_list.append(seq_id)\n",
    "\n",
    "    pad_len = PAD_PERCENTILE\n",
    "    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(feature_cols))\n",
    "    \n",
    "    # Convert to arrays\n",
    "    id_list = np.array(id_list)\n",
    "    X_list_all = pad_sequences_torch(X_list, maxlen=pad_len, padding='pre', truncating='pre')\n",
    "    y_list_all = np.eye(len(le.classes_))[y_list].astype(np.float32)\n",
    "    \n",
    "    print(f\"✓ Data prepared: {X_list_all.shape[0]} sequences, max length {pad_len}\")\n",
    "\n",
    "    # Initialize augmenter\n",
    "    augmenter = Augment(\n",
    "        p_jitter=0.9844818619033621, \n",
    "        sigma=0.03291295776089293, \n",
    "        scale_range=(0.7542342630597011, 1.1625052821731077),\n",
    "        p_dropout=0.41782786013520684, \n",
    "        p_moda=0.3910622476959722, \n",
    "        drift_std=0.0040285239353308015, \n",
    "        drift_max=0.3929358950258158,\n",
    "        p_time_warp=0.3, \n",
    "        warp_factor=0.1, \n",
    "        p_freq_noise=0.3, \n",
    "        freq_sigma=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T16:18:54.441154Z",
     "iopub.status.busy": "2025-07-31T16:18:54.440900Z",
     "iopub.status.idle": "2025-07-31T18:20:44.254778Z",
     "shell.execute_reply": "2025-07-31T18:20:44.253657Z",
     "shell.execute_reply.started": "2025-07-31T16:18:54.441124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Training Loop (GPU Optimized)\n",
    "if TRAIN:\n",
    "    EPOCHS = 125\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "    models = []\n",
    "    fold_scores = []\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(random_state)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(id_list, np.argmax(y_list_all, axis=1))):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Fold {fold + 1}/{FOLDS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_list = X_list_all[train_idx]\n",
    "        train_y_list = y_list_all[train_idx]\n",
    "        val_list = X_list_all[val_idx]\n",
    "        val_y_list = y_list_all[val_idx]\n",
    "        \n",
    "        print(f\"Train samples: {len(train_list)}, Val samples: {len(val_list)}\")\n",
    "\n",
    "        # Create datasets with optimized data loading\n",
    "        train_dataset = CMI3Dataset(train_list, train_y_list, maxlen, \n",
    "                                  mode=\"train\", imu_dim=len(imu_cols), augment=augmenter)\n",
    "        \n",
    "        # Adjust num_workers based on GPU capability\n",
    "        num_workers = 8 if COMPILE_SUPPORTED else 4\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers,\n",
    "            drop_last=True, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if num_workers > 0 else False\n",
    "        )\n",
    "\n",
    "        val_dataset = CMI3Dataset(val_list, val_y_list, maxlen, mode=\"val\")\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            drop_last=True, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if num_workers > 0 else False\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model = TwoBranchModel(\n",
    "            maxlen, len(imu_cols), len(tof_cols), len(le.classes_), fft_bins=fft_bins\n",
    "        ).to(device)\n",
    "        \n",
    "        # Compile model for better performance (only if supported)\n",
    "        if COMPILE_SUPPORTED and hasattr(torch, 'compile'):\n",
    "            try:\n",
    "                model = torch.compile(model)\n",
    "                print(f\"✓ Model compiled for fold {fold+1}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Compilation failed for fold {fold+1}, using eager mode: {e}\")\n",
    "        else:\n",
    "            print(f\"✓ Using eager mode for fold {fold+1}\")\n",
    "        \n",
    "        # Initialize training components\n",
    "        ema = EMA(model, decay=0.999)\n",
    "        \n",
    "        # Use fused optimizer only if CUDA capability >= 7.0\n",
    "        if COMPILE_SUPPORTED:\n",
    "            optimizer = Adam(model.parameters(), lr=LR_INIT, weight_decay=WD, fused=True)\n",
    "        else:\n",
    "            optimizer = Adam(model.parameters(), lr=LR_INIT, weight_decay=WD)\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        warmup = epochs_warmup * steps_per_epoch\n",
    "        nsteps = EPOCHS * steps_per_epoch\n",
    "        scheduler = CosineLRScheduler(\n",
    "            optimizer, \n",
    "            warmup_t=warmup, \n",
    "            warmup_lr_init=warmup_lr_init, \n",
    "            warmup_prefix=True,\n",
    "            t_initial=(nsteps - warmup), \n",
    "            lr_min=lr_min\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "        scaler_amp = GradScaler() if USE_AMP else None\n",
    "\n",
    "        # Training variables\n",
    "        best_val_score = 0.0\n",
    "        i_scheduler = 0\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_preds = []\n",
    "            train_targets = []\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Fold {fold+1}, Epoch {epoch+1}/{EPOCHS}\")\n",
    "            for batch_idx, (X, y) in enumerate(pbar):\n",
    "                X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        logits = model(X)\n",
    "                        loss = F.cross_entropy(logits, y.argmax(dim=1))\n",
    "                    \n",
    "                    scaler_amp.scale(loss).backward()\n",
    "                    \n",
    "                    if (batch_idx + 1) % ACCUM_STEPS == 0:\n",
    "                        scaler_amp.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        scaler_amp.step(optimizer)\n",
    "                        scaler_amp.update()\n",
    "                else:\n",
    "                    logits = model(X)\n",
    "                    loss = F.cross_entropy(logits, y.argmax(dim=1))\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (batch_idx + 1) % ACCUM_STEPS == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                ema.update(model)\n",
    "                \n",
    "                # Collect predictions\n",
    "                with torch.no_grad():\n",
    "                    train_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                    train_targets.extend(y.argmax(dim=1).cpu().numpy())\n",
    "                \n",
    "                scheduler.step(i_scheduler)\n",
    "                i_scheduler += 1\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "                })\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            ema.apply_shadow(model)  # Use EMA weights for validation\n",
    "            \n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X, y in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                    # Augmented validation with sensor dropout\n",
    "                    half = BATCH_SIZE // 2\n",
    "                    x_front = X[:half].to(device, non_blocking=True)\n",
    "                    x_back = X[half:].clone()\n",
    "                    x_back[:, :, 7:] = 0.0  # Sensor dropout\n",
    "                    x_back = x_back.to(device, non_blocking=True)\n",
    "                    X_val = torch.cat([x_front, x_back], dim=0)\n",
    "                    y = y.to(device, non_blocking=True)\n",
    "                    \n",
    "                    if USE_AMP:\n",
    "                        with autocast():\n",
    "                            logits = model(X_val)\n",
    "                            loss = F.cross_entropy(logits, y.argmax(dim=1))\n",
    "                    else:\n",
    "                        logits = model(X_val)\n",
    "                        loss = F.cross_entropy(logits, y.argmax(dim=1))\n",
    "                    \n",
    "                    val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                    val_targets.extend(y.argmax(dim=1).cpu().numpy())\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            ema.restore(model)  # Restore original weights\n",
    "\n",
    "            # Calculate metrics\n",
    "            train_acc = CompetitionMetric().calculate_hierarchical_f1(\n",
    "                pd.DataFrame({'gesture': le.classes_[train_targets]}),\n",
    "                pd.DataFrame({'gesture': le.classes_[train_preds]})\n",
    "            )\n",
    "            val_acc = CompetitionMetric().calculate_hierarchical_f1(\n",
    "                pd.DataFrame({'gesture': le.classes_[val_targets]}),\n",
    "                pd.DataFrame({'gesture': le.classes_[val_preds]})\n",
    "            )\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train F1: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val F1: {val_acc:.4f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if early_stopping(val_loss, model):\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_score:\n",
    "                best_val_score = val_acc\n",
    "                ema.apply_shadow(model)  # Use EMA weights for saving\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'model_config': {\n",
    "                        'pad_len': maxlen,\n",
    "                        'imu_dim_raw': len(imu_cols),\n",
    "                        'tof_dim': len(tof_cols),\n",
    "                        'n_classes': len(le.classes_),\n",
    "                        'fft_bins': fft_bins\n",
    "                    },\n",
    "                    'val_score': val_acc,\n",
    "                    'epoch': epoch\n",
    "                }, EXPORT_DIR / f\"gesture_two_branch_fold{fold}.pth\")\n",
    "                ema.restore(model)\n",
    "\n",
    "        fold_scores.append(best_val_score)\n",
    "        models.append(model)\n",
    "        print(f\"Fold {fold + 1} completed | Best Val F1: {best_val_score:.4f}\")\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Final results\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    std_score = np.std(fold_scores)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Cross-validation scores: {fold_scores}\")\n",
    "    print(f\"Mean CV Score: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "    print(f\"Models saved in: {EXPORT_DIR}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:02:54.217870Z",
     "iopub.status.busy": "2025-07-31T19:02:54.217520Z",
     "iopub.status.idle": "2025-07-31T19:02:54.275224Z",
     "shell.execute_reply": "2025-07-31T19:02:54.274409Z",
     "shell.execute_reply.started": "2025-07-31T19:02:54.217848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Prepare Models for Inference (Using Saved Models)\n",
    "print(\"Preparing models for inference...\")\n",
    "\n",
    "if TRAIN:\n",
    "    model_dir = EXPORT_DIR\n",
    "    print(\"Using artefacts from training session\")\n",
    "else:\n",
    "    model_dir = PRETRAINED_DIR\n",
    "    print(\"INFERENCE MODE – loading artefacts from\", model_dir)\n",
    "\n",
    "# Load saved artifacts consistently\n",
    "feature_cols = np.load(model_dir / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len = int(np.load(model_dir / \"sequence_maxlen.npy\"))\n",
    "scaler = joblib.load(model_dir / \"scaler.pkl\")\n",
    "gesture_classes = np.load(model_dir / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "print(f\"✓ Loaded feature columns: {len(feature_cols)}\")\n",
    "print(f\"✓ Sequence max length: {pad_len}\")\n",
    "print(f\"✓ Classes: {len(gesture_classes)}\")\n",
    "\n",
    "# Separate IMU and TOF columns\n",
    "imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "# Allowlist the numpy scalar for safe loading\n",
    "import torch.serialization\n",
    "import numpy.core.multiarray\n",
    "torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])\n",
    "\n",
    "# Load trained models\n",
    "MODELS = [f'gesture_two_branch_fold{i}.pth' for i in range(5)]\n",
    "models = []\n",
    "\n",
    "print(\"Loading trained models...\")\n",
    "for i, model_path in enumerate(MODELS):\n",
    "    full_path = model_dir / model_path\n",
    "    if full_path.exists():\n",
    "        checkpoint = torch.load(full_path, map_location=device)\n",
    "        \n",
    "        # Extract model configuration\n",
    "        config = checkpoint['model_config']\n",
    "        model = TwoBranchModel(**config).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        models.append(model)\n",
    "        print(f\"✓ Loaded fold {i} model (Val F1: {checkpoint.get('val_score', 'N/A'):.4f})\")\n",
    "    else:\n",
    "        print(f\"✗ Model {model_path} not found\")\n",
    "\n",
    "if len(models) == 0:\n",
    "    raise FileNotFoundError(\"No trained models found in the directory\")\n",
    "\n",
    "print(f\"✓ {len(models)} models loaded successfully\")\n",
    "\n",
    "# Enhanced prediction function with proper ensemble\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced prediction function with proper model ensemble\n",
    "    \"\"\"\n",
    "    global models, gesture_classes, scaler, feature_cols, pad_len\n",
    "    \n",
    "    # Convert polars to pandas\n",
    "    df_seq = sequence.to_pandas()\n",
    "    \n",
    "    # Preprocess sequence\n",
    "    mat = preprocess_sequence(df_seq, feature_cols, scaler)\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded = pad_sequences_torch([mat], maxlen=pad_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Convert to tensor and move to GPU\n",
    "    x = torch.FloatTensor(padded).to(device)\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    with torch.no_grad():\n",
    "        ensemble_probs = None\n",
    "        \n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with autocast():\n",
    "                    logits = model(x)\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "            else:\n",
    "                logits = model(x)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            if ensemble_probs is None:\n",
    "                ensemble_probs = probs\n",
    "            else:\n",
    "                ensemble_probs += probs\n",
    "        \n",
    "        if ensemble_probs is None:\n",
    "            raise ValueError(\"No models available for prediction\")\n",
    "        \n",
    "        ensemble_probs /= len(models)\n",
    "        predicted_idx = ensemble_probs.argmax(dim=1).item()\n",
    "    \n",
    "    return str(gesture_classes[predicted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T18:59:20.733480Z",
     "iopub.status.busy": "2025-07-31T18:59:20.732941Z",
     "iopub.status.idle": "2025-07-31T18:59:21.809478Z",
     "shell.execute_reply": "2025-07-31T18:59:21.808424Z",
     "shell.execute_reply.started": "2025-07-31T18:59:20.733456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Kaggle API Setup\n",
    "if  TRAIN:\n",
    "    print(\"Setting up Kaggle inference server...\")\n",
    "    try:\n",
    "        import kaggle_evaluation.cmi_inference_server\n",
    "        inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "        \n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            print(\"Running in competition mode...\")\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            print(\"Running local gateway...\")\n",
    "            inference_server.run_local_gateway(\n",
    "                data_paths=(\n",
    "                    '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "                    '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "                )\n",
    "            )\n",
    "    except ImportError:\n",
    "        print(\"Kaggle evaluation module not available - skipping inference server setup\")\n",
    "        print(\"Models are loaded and predict() function is ready for manual testing\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7771623,
     "sourceId": 12328761,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
