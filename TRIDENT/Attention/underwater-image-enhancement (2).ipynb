{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T, models as tv_models\n",
    "from PIL import Image\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "WORK_DIR = Path(\"/kaggle/working/\")\n",
    "MODELS_DIR = WORK_DIR / \"models\"\n",
    "CHECKPOINTS_DIR = WORK_DIR / \"checkpoints\"\n",
    "OUTPUT_DIR = WORK_DIR / \"outputs\"\n",
    "for p in (MODELS_DIR, CHECKPOINTS_DIR, OUTPUT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_DIR = \"/kaggle/input/trident/DATA\"\n",
    "ICAM_DIR = os.path.join(DATASET_DIR, \"Icam\")\n",
    "ICLEAN_DIR = os.path.join(DATASET_DIR, \"Iclean\")\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "MAX_EPOCHS = 80\n",
    "STAGE1_EPOCHS = 10\n",
    "SAVE_PTH_PERIOD = 10\n",
    "SEED = 42\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PairedCLAHEImageDataset(Dataset):\n",
    "    def __init__(self, in_files, gt_files, img_size=256, apply_clahe=True):\n",
    "        assert len(in_files) == len(gt_files)\n",
    "        self.in_files = in_files\n",
    "        self.gt_files = gt_files\n",
    "        self.img_size = img_size\n",
    "        self.apply_clahe = apply_clahe\n",
    "        self.to_tensor = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.in_files)\n",
    "\n",
    "    def apply_clahe_rgb(self, pil_img):\n",
    "        img = np.array(pil_img)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        cl = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(l)\n",
    "        lab = cv2.merge((cl, a, b))\n",
    "        rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        return Image.fromarray(rgb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_p = Image.open(self.in_files[idx]).convert(\"RGB\")\n",
    "        tgt_p = Image.open(self.gt_files[idx]).convert(\"RGB\")\n",
    "\n",
    "        inp_p = inp_p.resize((self.img_size, self.img_size), Image.BICUBIC)\n",
    "        tgt_p = tgt_p.resize((self.img_size, self.img_size), Image.BICUBIC)\n",
    "\n",
    "        if self.apply_clahe:\n",
    "            try:\n",
    "                inp_p = self.apply_clahe_rgb(inp_p)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return self.to_tensor(inp_p), self.to_tensor(tgt_p)\n",
    "\n",
    "\n",
    "def get_paired_file_lists(icam_dir, iclean_dir):\n",
    "    icam = sorted(glob(os.path.join(icam_dir, \"*\")))\n",
    "    iclean = sorted(glob(os.path.join(iclean_dir, \"*\")))\n",
    "    icam_map = {os.path.basename(p): p for p in icam}\n",
    "    iclean_map = {os.path.basename(p): p for p in iclean}\n",
    "    common = sorted(list(set(icam_map.keys()) & set(iclean_map.keys())))\n",
    "    return [icam_map[k] for k in common], [iclean_map[k] for k in common]\n",
    "\n",
    "\n",
    "icam_list, iclean_list = get_paired_file_lists(ICAM_DIR, ICLEAN_DIR)\n",
    "total_pairs = len(icam_list)\n",
    "print(\"Found total paired images:\", total_pairs)\n",
    "\n",
    "if NUM_SAMPLES is None or NUM_SAMPLES <= 0 or NUM_SAMPLES > total_pairs:\n",
    "    NUM_SAMPLES = total_pairs\n",
    "\n",
    "indices = list(range(total_pairs))\n",
    "random.shuffle(indices)\n",
    "\n",
    "selected = indices[:NUM_SAMPLES]\n",
    "leftover = indices[NUM_SAMPLES:]  # for inference/export demo\n",
    "\n",
    "icam_sel = [icam_list[i] for i in selected]\n",
    "iclean_sel = [iclean_list[i] for i in selected]\n",
    "\n",
    "infer_icam = [icam_list[i] for i in leftover]\n",
    "infer_iclean = [iclean_list[i] for i in leftover]\n",
    "\n",
    "# Train / Val / Test Split\n",
    "n_train = int(0.8 * NUM_SAMPLES)\n",
    "n_val = int(0.1 * NUM_SAMPLES)\n",
    "n_test = NUM_SAMPLES - n_train - n_val\n",
    "\n",
    "train_in = icam_sel[:n_train]\n",
    "train_gt = iclean_sel[:n_train]\n",
    "val_in = icam_sel[n_train:n_train + n_val]\n",
    "val_gt = iclean_sel[n_train:n_train + n_val]\n",
    "test_in = icam_sel[n_train + n_val:]\n",
    "test_gt = iclean_sel[n_train + n_val:]\n",
    "\n",
    "print(\"Train / Val / Test =\", len(train_in), len(val_in), len(test_in))\n",
    "print(\"Leftover for inference:\", len(infer_icam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnderWaterDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_pairs, val_pairs, test_pairs, infer_pairs,\n",
    "                 img_size=256, batch_size=8, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_pairs = train_pairs\n",
    "        self.val_pairs = val_pairs\n",
    "        self.test_pairs = test_pairs\n",
    "        self.infer_pairs = infer_pairs\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_ds = PairedCLAHEImageDataset(*self.train_pairs, self.img_size, apply_clahe=True)\n",
    "        self.val_ds = PairedCLAHEImageDataset(*self.val_pairs, self.img_size, apply_clahe=False)\n",
    "        self.test_ds = PairedCLAHEImageDataset(*self.test_pairs, self.img_size, apply_clahe=False)\n",
    "        self.infer_ds = PairedCLAHEImageDataset(*self.infer_pairs, self.img_size, apply_clahe=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.infer_ds, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "dm = UnderWaterDataModule(\n",
    "    train_pairs=(train_in, train_gt),\n",
    "    val_pairs=(val_in, val_gt),\n",
    "    test_pairs=(test_in, test_gt),\n",
    "    infer_pairs=(infer_icam, infer_iclean),\n",
    "    img_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "print(\"DataModule ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg = self.fc(self.avg_pool(x))\n",
    "        mx = self.fc(self.max_pool(x))\n",
    "        out = self.sigmoid(avg + mx)\n",
    "        return x * out\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # channel-wise avg and max\n",
    "        avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        mx, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg, mx], dim=1)\n",
    "        out = self.sigmoid(self.conv(cat))\n",
    "        return x * out\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ca(x)\n",
    "        x = self.sa(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"CBAM modules defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet34_UNet_CBAM(nn.Module):\n",
    "    def __init__(self, pretrained=True, cbam_reduction=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        resnet = tv_models.resnet34(weights=tv_models.ResNet34_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "\n",
    "        # Encoder layers (we'll keep references for skip connections)\n",
    "        self.conv1 = resnet.conv1  # out: 64, /2 (after conv)\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "\n",
    "        self.layer1 = resnet.layer1  # 64  (after maxpool)\n",
    "        self.layer2 = resnet.layer2  # 128\n",
    "        self.layer3 = resnet.layer3  # 256\n",
    "        self.layer4 = resnet.layer4  # 512\n",
    "\n",
    "        # CBAM on deepest features\n",
    "        self.cbam = CBAM(channels=512, reduction=cbam_reduction, kernel_size=7)\n",
    "\n",
    "        # Decoder conv blocks helper\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(p=dropout)\n",
    "            )\n",
    "\n",
    "        # Decoder (upsample & concat with corresponding skip)\n",
    "        self.up3 = conv_block(512 + 256, 256)\n",
    "        self.up2 = conv_block(256 + 128, 128)\n",
    "        self.up1 = conv_block(128 + 64, 64)\n",
    "        self.up0 = conv_block(64 + 64, 64)\n",
    "        self.up_final = conv_block(64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.relu(self.bn1(self.conv1(x)))  # [B,64,H/2,W/2]\n",
    "        x1 = self.layer1(self.maxpool(x0))  # [B,64,H/4,W/4]\n",
    "        x2 = self.layer2(x1)  # [B,128,H/8,W/8]\n",
    "        x3 = self.layer3(x2)  # [B,256,H/16,W/16]\n",
    "        x4 = self.layer4(x3)  # [B,512,H/32,W/32]\n",
    "\n",
    "        # CBAM applied to deep feature\n",
    "        z = self.cbam(x4)\n",
    "\n",
    "        # Decoder path - upsample + concat + conv block\n",
    "        u3 = F.interpolate(z, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        u3 = self.up3(torch.cat([u3, x3], dim=1))\n",
    "\n",
    "        u2 = F.interpolate(u3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        u2 = self.up2(torch.cat([u2, x2], dim=1))\n",
    "\n",
    "        u1 = F.interpolate(u2, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        u1 = self.up1(torch.cat([u1, x1], dim=1))\n",
    "\n",
    "        u0 = F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        u0 = self.up0(torch.cat([u0, x0], dim=1))\n",
    "\n",
    "        uF = F.interpolate(u0, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        uF = self.up_final(uF)\n",
    "\n",
    "        out = torch.sigmoid(self.final_conv(uF))  # ensure 0-1\n",
    "        return out\n",
    "\n",
    "\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    \"\"\"✅ FIXED: Correct class name in super() call\"\"\"\n",
    "    def __init__(self, window_size=11, channel=3):\n",
    "        super(SSIMLoss, self).__init__()  # ✅ Fixed from super(SSIMLoss, self)\n",
    "        self.window_size = window_size\n",
    "        self.channel = channel\n",
    "\n",
    "        sigma = 1.5\n",
    "        coords = torch.arange(window_size).float() - window_size // 2\n",
    "        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "        g = g / g.sum()\n",
    "\n",
    "        window = g[:, None] * g[None, :]\n",
    "        self.register_buffer(\"window\", window.expand(channel, 1, window_size, window_size).clone())\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        window = self.window.to(img1.device)\n",
    "\n",
    "        mu1 = F.conv2d(img1, window, padding=self.window_size // 2, groups=self.channel)\n",
    "        mu2 = F.conv2d(img2, window, padding=self.window_size // 2, groups=self.channel)\n",
    "\n",
    "        mu1_sq = mu1 ** 2\n",
    "        mu2_sq = mu2 ** 2\n",
    "        mu12 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = F.conv2d(img1 * img1, window, padding=self.window_size // 2, groups=self.channel) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(img2 * img2, window, padding=self.window_size // 2, groups=self.channel) - mu2_sq\n",
    "        sigma12 = F.conv2d(img1 * img2, window, padding=self.window_size // 2, groups=self.channel) - mu12\n",
    "\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "\n",
    "        ssim = ((2 * mu12 + C1) * (2 * sigma12 + C2)) / \\\n",
    "               ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "        return torch.clamp((1 - ssim) / 2, 0, 1).mean()\n",
    "\n",
    "\n",
    "print(\"SSIM Loss defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = tv_models.vgg19(weights=tv_models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        self.slice = nn.Sequential(*list(vgg[:16])).eval()\n",
    "        for p in self.slice.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_vgg = self.slice(x)\n",
    "        y_vgg = self.slice(y)\n",
    "        return F.l1_loss(x_vgg, y_vgg)\n",
    "\n",
    "\n",
    "print(\"VGG Perceptual Loss defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RestorationLitModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.model = ResNet34_UNet_CBAM(pretrained=True, cbam_reduction=16, dropout=0.1)\n",
    "        self.perc_loss = VGGPerceptualLoss()\n",
    "        self.ssim_loss = SSIMLoss()  # ✅ FIXED: Instantiate the class\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.get(\"lr\", LR),\n",
    "            weight_decay=self.hparams.get(\"weight_decay\", WEIGHT_DECAY)\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode='min', factor=0.5, patience=4, verbose=True\n",
    "        )\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val/total\"}}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inp, tgt = batch\n",
    "        pred = self(inp)\n",
    "\n",
    "        l1 = F.l1_loss(pred, tgt)\n",
    "        perc = self.perc_loss(pred, tgt)\n",
    "        ssim = self.ssim_loss(pred, tgt)  # ✅ Already returns loss (1 - SSIM)\n",
    "        total = l1 + 0.1 * perc + 0.1 * ssim\n",
    "\n",
    "        self.log(\"train/l1\", l1, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/perc\", perc, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/ssim\", ssim, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/total\", total, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inp, tgt = batch\n",
    "        pred = self(inp)\n",
    "\n",
    "        l1 = F.l1_loss(pred, tgt)\n",
    "        perc = self.perc_loss(pred, tgt)\n",
    "        ssim = self.ssim_loss(pred, tgt)\n",
    "        total = l1 + 0.1 * perc + 0.1 * ssim\n",
    "\n",
    "        self.log(\"val/l1\", l1, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/perc\", perc, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/ssim\", ssim, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/total\", total, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inp, tgt = batch\n",
    "        pred = self(inp)\n",
    "        loss = F.l1_loss(pred, tgt)\n",
    "        self.log(\"test/loss\", loss)\n",
    "        return {\"test_loss\": loss}\n",
    "\n",
    "\n",
    "print(\"Lightning Module defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not to train cell starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PeriodicWeightsCallback(Callback):\n",
    "    \"\"\"Save model.state_dict() (weights-only .pth) every 'period' epochs and when best model updates.\"\"\"\n",
    "    def __init__(self, save_dir=MODELS_DIR, period=SAVE_PTH_PERIOD):\n",
    "        super().__init__()\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.period = period\n",
    "        self.best_val = float(\"inf\")\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch + 1\n",
    "        metrics = trainer.callback_metrics\n",
    "        \n",
    "        # Save every period\n",
    "        if epoch % self.period == 0:\n",
    "            dest = self.save_dir / f\"weights_epoch_{epoch:03d}.pth\"\n",
    "            torch.save(pl_module.model.state_dict(), str(dest))\n",
    "            print(f\"\\nSaved weights-only snapshot: {dest}\")\n",
    "        \n",
    "        # Save best by val loss\n",
    "        val_loss = metrics.get(\"val/total\")\n",
    "        if val_loss is not None:\n",
    "            try:\n",
    "                val_loss_val = float(val_loss)\n",
    "                if val_loss_val < self.best_val:\n",
    "                    self.best_val = val_loss_val\n",
    "                    dest = self.save_dir / f\"best_weights_epoch_{epoch:03d}.pth\"\n",
    "                    torch.save(pl_module.model.state_dict(), str(dest))\n",
    "                    print(f\"\\nSaved NEW best weights-only snapshot: {dest}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "class PrintEveryNEpochsCallback(Callback):\n",
    "    def __init__(self, n=5):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch + 1\n",
    "        if epoch % self.n == 0 or epoch == 1:\n",
    "            metrics = trainer.callback_metrics\n",
    "            tl = metrics.get(\"train/total\")\n",
    "            l1 = metrics.get(\"train/l1\")\n",
    "            perc = metrics.get(\"train/perc\")\n",
    "            ssimv = metrics.get(\"train/ssim\")\n",
    "            vl = metrics.get(\"val/total\")\n",
    "            vl1 = metrics.get(\"val/l1\")\n",
    "            vperc = metrics.get(\"val/perc\")\n",
    "            vssim = metrics.get(\"val/ssim\")\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {epoch:03d} SUMMARY:\")\n",
    "            if tl is not None:\n",
    "                print(f\"  Train -> Total: {float(tl):.4f}  L1: {float(l1):.4f}  Perc: {float(perc):.4f}  SSIM: {float(ssimv):.4f}\")\n",
    "            if vl is not None:\n",
    "                print(f\"  Val   -> Total: {float(vl):.4f}  L1: {float(vl1):.4f}  Perc: {float(vperc):.4f}  SSIM: {float(vssim):.4f}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "print(\"Callbacks defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hparams = {\"lr\": LR, \"weight_decay\": WEIGHT_DECAY}\n",
    "model = RestorationLitModel(hparams)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=str(CHECKPOINTS_DIR),\n",
    "    filename=\"best-{epoch:02d}-{val/total:.4f}\",\n",
    "    monitor=\"val/total\",  # ✅ FIXED: Match the logged metric name\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val/total\",  # ✅ FIXED: Match the logged metric name\n",
    "    patience=10,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\"logs\", name=\"uw_rest_cbam\")\n",
    "\n",
    "periodic_cb = PeriodicWeightsCallback(\n",
    "    save_dir=MODELS_DIR,\n",
    "    period=SAVE_PTH_PERIOD\n",
    ")\n",
    "\n",
    "printer_cb = PrintEveryNEpochsCallback(n=5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\" if torch.cuda.is_available() else 32,  # ✅ Updated precision syntax\n",
    "    callbacks=[checkpoint_callback, early_stop, periodic_cb, printer_cb],\n",
    "    logger=csv_logger,\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val/total\",  # ✅ FIXED: Match the logged metric name\n",
    "    patience=10,  # Stops if no improvement for 10 epochs\n",
    "    mode=\"min\",  # We want to minimize the loss\n",
    "    verbose=True,  # Print messages when early stopping is triggered\n",
    "    min_delta=0.0001  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ Use the DataModule (dm)\n",
    "last_ckpt = CHECKPOINTS_DIR / \"last.ckpt\"\n",
    "if last_ckpt.exists():\n",
    "    print(\"Resuming from checkpoint:\", last_ckpt)\n",
    "    trainer.fit(model, datamodule=dm, ckpt_path=str(last_ckpt))\n",
    "else:\n",
    "    print(\"Training from scratch.\")\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_ckpt = checkpoint_callback.best_model_path\n",
    "if best_ckpt:\n",
    "    shutil.copy(best_ckpt, MODELS_DIR / \"best_model_full.ckpt\")\n",
    "    print(\"Copied best full ckpt to models dir:\", MODELS_DIR / \"best_model_full.ckpt\")\n",
    "\n",
    "# Zip weights-only snapshots directory\n",
    "zipf = WORK_DIR / \"weights_snapshots.zip\"\n",
    "with zipfile.ZipFile(zipf, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in MODELS_DIR.glob(\"*.pth\"):\n",
    "        z.write(p, arcname=p.name)\n",
    "print(\"Zipped weights snapshots to:\", zipf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Path to CSV (update as needed)\n",
    "metrics_path = \"/kaggle/working/logs/uw_rest_cbam/version_7/metrics.csv\"\n",
    "if not os.path.exists(metrics_path):\n",
    "    print(\"Metrics CSV not found. Training log may be incomplete.\")\n",
    "else:\n",
    "    df = pd.read_csv(metrics_path)\n",
    "    loss_cols = [col for col in df.columns if 'train_loss' in col or 'val_loss' in col]\n",
    "    if not loss_cols:\n",
    "        print(\"No train/val loss columns found yet. Model may still be training, or logger needs updating.\")\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        for col in loss_cols:\n",
    "            plt.plot(df[col].dropna().values, label=col)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training/Validation Loss Curves\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not to run cell ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Define path to checkpoint in Kaggle input\n",
    "checkpoint_path = Path(\"/kaggle/input/test-1/best_weights_epoch_041.pth\")\n",
    "\n",
    "if not checkpoint_path.is_file():\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "# Instantiate your model exactly as in your notebook\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "\n",
    "# Load weights (state dict) from checkpoint\n",
    "state = torch.load(str(checkpoint_path), map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "infer_model.to(device)\n",
    "\n",
    "print(\"Model loaded and ready for inference on device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Assuming these variables/constants are imported or declared before:\n",
    "# dm - your lightning DataModule\n",
    "# ResNet34_UNet_CBAM - your model class\n",
    "# OUTPUT_DIR, MODELS_DIR, CHECKPOINTS_DIR, BATCH_SIZE, ssim_fn (torchmetrics)\n",
    "\n",
    "# Path to weights .pth snapshot file you want to load for inference\n",
    "weights_path = \"/kaggle/input/test-1/best_weights_epoch_041.pth\"\n",
    "weights_path = Path(weights_path)\n",
    "\n",
    "if not weights_path.is_file():\n",
    "    raise FileNotFoundError(f\"No .pth weights file found at: {weights_path}\")\n",
    "\n",
    "print(f\"Loading weights-only for inference: {weights_path}\")\n",
    "\n",
    "# Instantiate model and load weights\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "state = torch.load(weights_path, map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "infer_model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Run inference on predict dataloader\n",
    "infer_loader = dm.predict_dataloader()\n",
    "results = []\n",
    "\n",
    "print(\"Running inference...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(infer_loader):\n",
    "        inp, tgt = batch\n",
    "        inp = inp.to(device)\n",
    "        pred = infer_model(inp)\n",
    "\n",
    "        results.append({\n",
    "            'input': inp.cpu(),\n",
    "            'pred': pred.cpu(),\n",
    "            'target': tgt.cpu(),\n",
    "            'batch_idx': batch_idx\n",
    "        })\n",
    "\n",
    "print(f\"Ran inference on {len(results)} batches ({len(results)*BATCH_SIZE} images).\")\n",
    "\n",
    "# Utility to convert tensor to numpy image for visualization\n",
    "def tensor_to_np(t):\n",
    "    if t.dim() == 4:\n",
    "        t = t[0]\n",
    "    return torch.clamp(t, 0, 1).permute(1, 2, 0).numpy()\n",
    "\n",
    "# Visualize first 30 batches\n",
    "n_show = min(30, len(results))\n",
    "rows = []\n",
    "for i in range(n_show):\n",
    "    batch_result = results[i]\n",
    "    inp = batch_result['input']\n",
    "    pred = batch_result['pred']\n",
    "    tgt = batch_result['target']\n",
    "\n",
    "    inp_img = tensor_to_np(inp)\n",
    "    pred_img = tensor_to_np(pred)\n",
    "    tgt_img = tensor_to_np(tgt)\n",
    "\n",
    "    row = np.concatenate([inp_img, pred_img, tgt_img], axis=1)\n",
    "    rows.append(row)\n",
    "\n",
    "if rows:\n",
    "    grid = np.concatenate(rows, axis=0)\n",
    "    out_file = OUTPUT_DIR / \"inference_sample_grid.png\"\n",
    "    plt.figure(figsize=(15, 5 * n_show))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Input | Restored | Ground Truth\", fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"✅ Saved inference sample grid to: {out_file}\")\n",
    "\n",
    "# Save individual predictions for first 20 batches\n",
    "print(\"\\nSaving individual predictions...\")\n",
    "pred_dir = OUTPUT_DIR / \"predictions\"\n",
    "pred_dir.mkdir(exist_ok=True)\n",
    "\n",
    "saved_count = 0\n",
    "for batch_idx, batch_result in enumerate(results[:20]):\n",
    "    pred_batch = batch_result['pred']\n",
    "    for img_idx in range(pred_batch.shape[0]):\n",
    "        pred_img = tensor_to_np(pred_batch[img_idx:img_idx+1])\n",
    "        pred_pil = Image.fromarray((pred_img * 255).astype(np.uint8))\n",
    "        save_path = pred_dir / f\"pred_batch{batch_idx:03d}_img{img_idx:02d}.png\"\n",
    "        pred_pil.save(save_path)\n",
    "        saved_count += 1\n",
    "\n",
    "print(f\"✅ Saved {saved_count} individual predictions to: {pred_dir}\")\n",
    "\n",
    "# Calculate metrics PSNR and SSIM over test set\n",
    "print(\"\\nCalculating metrics on predictions...\")\n",
    "psnr_values = []\n",
    "ssim_values = []\n",
    "\n",
    "for batch_result in results:\n",
    "    pred_batch = batch_result['pred']\n",
    "    tgt_batch = batch_result['target']\n",
    "\n",
    "    for i in range(pred_batch.shape[0]):\n",
    "        pred_img = pred_batch[i:i+1].to(device)\n",
    "        tgt_img = tgt_batch[i:i+1].to(device)\n",
    "\n",
    "        mse = F.mse_loss(pred_img, tgt_img)\n",
    "        psnr = 10 * torch.log10(1.0 / mse)\n",
    "        psnr_values.append(psnr.item())\n",
    "\n",
    "        ssim_val = ssim_fn(pred_img, tgt_img, data_range=1.0)\n",
    "        ssim_values.append(ssim_val.item())\n",
    "\n",
    "avg_psnr = np.mean(psnr_values)\n",
    "avg_ssim = np.mean(ssim_values)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"INFERENCE METRICS:\")\n",
    "print(f\"  Average PSNR: {avg_psnr:.2f} dB\")\n",
    "print(f\"  Average SSIM: {avg_ssim:.4f}\")\n",
    "print(f\"  Total images evaluated: {len(psnr_values)}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save metrics to text file\n",
    "metrics_file = OUTPUT_DIR / \"inference_metrics.txt\"\n",
    "with open(metrics_file, 'w') as f:\n",
    "    f.write(\"Inference Metrics\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Model: {weights_path}\\n\")\n",
    "    f.write(f\"Number of images: {len(psnr_values)}\\n\")\n",
    "    f.write(f\"Average PSNR: {avg_psnr:.2f} dB\\n\")\n",
    "    f.write(f\"Average SSIM: {avg_ssim:.4f}\\n\")\n",
    "    f.write(f\"PSNR std: {np.std(psnr_values):.2f}\\n\")\n",
    "    f.write(f\"SSIM std: {np.std(ssim_values):.4f}\\n\")\n",
    "\n",
    "print(f\"✅ Saved metrics to: {metrics_file}\")\n",
    "\n",
    "print(\"\\nALL DONE! ✅\")\n",
    "print(f\"Check outputs in: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn\n",
    "\n",
    "# User params\n",
    "BATCH_SIZE = 8\n",
    "PLOT_LIMIT = 32\n",
    "\n",
    "def adaptive_red_boost(img):\n",
    "    r_mean = img[..., 0].mean()\n",
    "    g_mean = img[..., 1].mean()\n",
    "    b_mean = img[..., 2].mean()\n",
    "    if r_mean < 1:\n",
    "        factor = 2.0\n",
    "    else:\n",
    "        factor = (g_mean + b_mean) / (2 * r_mean)\n",
    "    factor = np.clip(factor, 1.2, 3.0)\n",
    "    out = img.copy()\n",
    "    out[..., 0] = np.clip(out[..., 0] * factor, 0, 255)\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "def gamma_correct(img, gamma=1.4):\n",
    "    img_corr = exposure.adjust_gamma(img / 255., gamma)\n",
    "    return (img_corr * 255).astype(np.uint8)\n",
    "\n",
    "def clahe_enhancement(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img_clahe = np.zeros_like(img)\n",
    "    for i in range(3):\n",
    "        img_clahe[..., i] = clahe.apply(img[..., i])\n",
    "    return img_clahe\n",
    "\n",
    "def simple_dehaze(img, omega=0.95, win_size=15):\n",
    "    norm_img = img.astype(np.float32) / 255.0\n",
    "    dark_channel = cv2.erode(np.min(norm_img, axis=2), np.ones((win_size, win_size)))\n",
    "    A = np.max(norm_img, axis=(0,1))\n",
    "    t = 1 - omega * dark_channel[..., np.newaxis]\n",
    "    t = np.clip(t, 0.1, 1)\n",
    "    J = (norm_img - A) / t + A\n",
    "    J = np.clip(J * 255, 0, 255).astype(np.uint8)\n",
    "    return J\n",
    "\n",
    "def tensor_to_img(t):\n",
    "    t = torch.clamp(t, 0, 1)\n",
    "    img = t.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(\"/kaggle/input/test-1/best_weights_epoch_041.pth\")\n",
    "if not checkpoint_path.is_file():\n",
    "    raise RuntimeError(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "\n",
    "print(f\"Loading weights-only for inference: {checkpoint_path}\")\n",
    "\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "infer_model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "full_loader = dm.predict_dataloader()\n",
    "\n",
    "results = []\n",
    "print(\"Running inference with metrics computation...\")\n",
    "psnr_values_batch = []\n",
    "ssim_values_batch = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(full_loader):\n",
    "        inp, tgt = batch\n",
    "        inp = inp.to(device)\n",
    "        pred = infer_model(inp)\n",
    "        pred_cpu = pred.cpu()\n",
    "\n",
    "        # Compute metrics batch-wise\n",
    "        for i in range(inp.shape[0]):\n",
    "            pred_img = pred[i:i+1]\n",
    "            tgt_img = tgt[i:i+1].to(device)\n",
    "            mse = F.mse_loss(pred_img, tgt_img)\n",
    "            psnr = 10 * torch.log10(1.0 / mse)\n",
    "            ssim_val = ssim_fn(pred_img, tgt_img, data_range=1.0)\n",
    "            psnr_values_batch.append(psnr.item())\n",
    "            ssim_values_batch.append(ssim_val.item())\n",
    "\n",
    "        print(f\"Batch {batch_idx}: PSNR = {np.mean(psnr_values_batch):.2f} dB, SSIM = {np.mean(ssim_values_batch):.4f}\")\n",
    "\n",
    "        # Process and store all images for visualization\n",
    "        for i in range(inp.shape[0]):\n",
    "            orig_img = tensor_to_img(inp[i])\n",
    "            gt_img = tensor_to_img(tgt[i])\n",
    "            pred_img_cpu = pred_cpu[i]\n",
    "\n",
    "            pred_img = tensor_to_img(pred_img_cpu)\n",
    "            clahe_img = clahe_enhancement(pred_img)\n",
    "            dehazed_img = simple_dehaze(clahe_img)\n",
    "            gamma_corrected = gamma_correct(dehazed_img, gamma=1.4)\n",
    "            red_boosted = adaptive_red_boost(gamma_corrected)\n",
    "            \n",
    "\n",
    "            results.append({\n",
    "                \"original\": orig_img,\n",
    "                \"ground_truth\": gt_img,\n",
    "                \"restored\": pred_img,\n",
    "                \"red_boosted\": red_boosted,\n",
    "                \"gamma_corrected\": gamma_corrected,\n",
    "                \"clahe_corrected\": clahe_img,\n",
    "                \"dehazed\": dehazed_img,\n",
    "            })\n",
    "\n",
    "        if len(results) >= PLOT_LIMIT:\n",
    "            break\n",
    "\n",
    "print(f\"Collected {len(results)} images for visualization.\")\n",
    "\n",
    "# Visualization grid: 7 images per row\n",
    "plt.figure(figsize=(20, PLOT_LIMIT * 3))\n",
    "titles = ['Camera Input', 'Ground Truth', 'Restored',\n",
    "          'Red Boosted', 'Gamma Corrected', 'CLAHE Contrast', 'Dehazed']\n",
    "\n",
    "for idx, res in enumerate(results[:PLOT_LIMIT]):\n",
    "    imgs = [res[t] for t in ['original', 'ground_truth', 'restored', 'red_boosted', 'gamma_corrected', 'clahe_corrected', 'dehazed']]\n",
    "\n",
    "    for col_idx, (img, title) in enumerate(zip(imgs, titles)):\n",
    "        plt.subplot(PLOT_LIMIT, 7, idx * 7 + col_idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        if idx == 0:\n",
    "            plt.title(title, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "from pathlib import Path\n",
    "\n",
    "# Parameters\n",
    "PLOT_LIMIT = 5  # Show 5 samples to keep plots clear\n",
    "\n",
    "# Define classical color correction functions\n",
    "def adaptive_red_boost(img):\n",
    "    r_mean = img[..., 0].mean()\n",
    "    g_mean = img[..., 1].mean()\n",
    "    b_mean = img[..., 2].mean()\n",
    "    factor = 2.0 if r_mean < 1 else (g_mean + b_mean) / (2 * r_mean)\n",
    "    factor = np.clip(factor, 1.1, 1.2)\n",
    "    out = img.copy()\n",
    "    out[..., 0] = np.clip(out[..., 0] * factor, 0, 255)\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "def gamma_correct(img, gamma=1.4):\n",
    "    img_corr = exposure.adjust_gamma(img / 255., gamma)\n",
    "    return (img_corr * 255).astype(np.uint8)\n",
    "\n",
    "def clahe_enhancement(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))\n",
    "    img_clahe = np.empty_like(img)\n",
    "    for i in range(3):\n",
    "        img_clahe[..., i] = clahe.apply(img[..., i])\n",
    "    return img_clahe\n",
    "\n",
    "def simple_dehaze(img, omega=0.95, win_size=15):\n",
    "    norm_img = img.astype(np.float32)/255.0\n",
    "    dark_channel = cv2.erode(np.min(norm_img, axis=2), np.ones((win_size, win_size)))\n",
    "    A = np.max(norm_img, axis=(0,1))\n",
    "    t = 1 - omega * dark_channel[..., np.newaxis]\n",
    "    t = np.clip(t, 0.1, 1)\n",
    "    J = (norm_img - A) / t + A\n",
    "    J = np.clip(J*255, 0, 255).astype(np.uint8)\n",
    "    return J\n",
    "\n",
    "def gray_world_white_balance(img):\n",
    "    img_float = img.astype(np.float32)\n",
    "    avgR = np.mean(img_float[..., 0])\n",
    "    avgG = np.mean(img_float[..., 1])\n",
    "    avgB = np.mean(img_float[..., 2])\n",
    "    avg = (avgR + avgG + avgB) / 3\n",
    "    scaleR, scaleG, scaleB = avg/avgR, avg/avgG, avg/avgB\n",
    "    img_balanced = img_float.copy()\n",
    "    img_balanced[..., 0] *= scaleR\n",
    "    img_balanced[..., 1] *= scaleG\n",
    "    img_balanced[..., 2] *= scaleB\n",
    "    return np.clip(img_balanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "def tensor_to_img(tensor):\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    img = tensor.detach().cpu().permute(1,2,0).numpy()\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "# Perceptual loss using VGG16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = vgg16(pretrained=True).features.eval().to(device)\n",
    "transform_vgg = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def perceptual_loss(img1, img2):\n",
    "    im1 = transform_vgg(img1).unsqueeze(0).to(device)\n",
    "    im2 = transform_vgg(img2).unsqueeze(0).to(device)\n",
    "    f1, f2 = vgg(im1), vgg(im2)\n",
    "    return F.l1_loss(f1, f2).item()\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint_path = Path(\"/kaggle/input/test-1/best_weights_epoch_041.pth\")\n",
    "if not checkpoint_path.is_file():\n",
    "    raise RuntimeError(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "infer_model.to(device)\n",
    "\n",
    "# Use your datamodule to prepare dataloader\n",
    "full_loader = dm.predict_dataloader()\n",
    "\n",
    "results = []\n",
    "\n",
    "metrics_before = {'psnr': [], 'ssim': [], 'perceptual': []}\n",
    "metrics_after = {'psnr': [], 'ssim': [], 'perceptual': []}\n",
    "\n",
    "import math\n",
    "\n",
    "# Run inference and corrections\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(full_loader):\n",
    "        inp, tgt = batch\n",
    "        inp = inp.to(device)\n",
    "        pred = infer_model(inp)\n",
    "        pred_cpu = pred.cpu()\n",
    "\n",
    "        for i in range(inp.size(0)):\n",
    "            orig_img = tensor_to_img(inp[i])\n",
    "            gt_img = tensor_to_img(tgt[i])\n",
    "            pred_img = tensor_to_img(pred_cpu[i])\n",
    "\n",
    "            # Metrics before correction\n",
    "            p_tensor, gt_tensor = pred[i:i+1].to(device), tgt[i:i+1].to(device)\n",
    "            mse = F.mse_loss(p_tensor, gt_tensor)\n",
    "            metrics_before['psnr'].append(10 * torch.log10(1.0 / mse).item())\n",
    "            metrics_before['ssim'].append(ssim_fn(p_tensor, gt_tensor).item())\n",
    "            metrics_before['perceptual'].append(perceptual_loss(pred_img, gt_img))\n",
    "\n",
    "            # Classical corrections\n",
    "            \n",
    "            \n",
    "            clahe_img = clahe_enhancement(pred_img)\n",
    "            #clahe_img = clahe_enhancement(clahe_img)\n",
    "            dehazed = simple_dehaze(clahe_img)\n",
    "            gamma_corr_img = gamma_correct(clahe_img, gamma=1.4)\n",
    "            wb_img = gray_world_white_balance(clahe_img)\n",
    "            red_boost_img = adaptive_red_boost(clahe_img)\n",
    "            \n",
    "            # Metrics after correction\n",
    "            gamma_tensor = torch.tensor(clahe_img / 255.).unsqueeze(0).permute(0,3,1,2).to(device)\n",
    "            mse_after = F.mse_loss(gamma_tensor, gt_tensor)\n",
    "            metrics_after['psnr'].append(10 * torch.log10(1.0 / mse_after).item())\n",
    "            metrics_after['ssim'].append(ssim_fn(gamma_tensor, gt_tensor).item())\n",
    "            metrics_after['perceptual'].append(perceptual_loss(gamma_corr_img, gt_img))\n",
    "\n",
    "            # Append for visualization\n",
    "            results.append({\n",
    "                'original': orig_img,\n",
    "                'ground_truth': gt_img,\n",
    "                'restored': pred_img,\n",
    "                'dehazed': dehazed,\n",
    "                'clahe_corrected': clahe_img,\n",
    "                'white_balanced': wb_img,\n",
    "                'red_boosted': red_boost_img,\n",
    "                'gamma_corrected': gamma_corr_img\n",
    "            })\n",
    "\n",
    "            if len(results) >= PLOT_LIMIT:\n",
    "                break\n",
    "        if len(results) >= PLOT_LIMIT:\n",
    "            break\n",
    "\n",
    "print(f\"Avg PSNR before corrections: {np.mean(metrics_before['psnr']):.3f} dB\")\n",
    "print(f\"Avg SSIM before corrections: {np.mean(metrics_before['ssim']):.4f}\")\n",
    "print(f\"Avg Perceptual loss before corrections: {np.mean(metrics_before['perceptual']):.4f}\")\n",
    "\n",
    "print(f\"Avg PSNR after corrections: {np.mean(metrics_after['psnr']):.3f} dB\")\n",
    "print(f\"Avg SSIM after corrections: {np.mean(metrics_after['ssim']):.4f}\")\n",
    "print(f\"Avg Perceptual loss after corrections: {np.mean(metrics_after['perceptual']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "from pathlib import Path\n",
    "\n",
    "# Parameters\n",
    "PLOT_LIMIT = 2000  # Show 5 samples to keep plots clear\n",
    "\n",
    "# Define classical color correction functions\n",
    "def adaptive_red_boost(img):\n",
    "    r_mean = img[..., 0].mean()\n",
    "    g_mean = img[..., 1].mean()\n",
    "    b_mean = img[..., 2].mean()\n",
    "    factor = 2.0 if r_mean < 1 else (g_mean + b_mean) / (2 * r_mean)\n",
    "    factor = np.clip(factor, 1.1, 1.2)\n",
    "    out = img.copy()\n",
    "    out[..., 0] = np.clip(out[..., 0] * factor, 0, 255)\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "def gamma_correct(img, gamma=1.4):\n",
    "    img_corr = exposure.adjust_gamma(img / 255., gamma)\n",
    "    return (img_corr * 255).astype(np.uint8)\n",
    "\n",
    "def clahe_enhancement(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))\n",
    "    img_clahe = np.empty_like(img)\n",
    "    for i in range(3):\n",
    "        img_clahe[..., i] = clahe.apply(img[..., i])\n",
    "    return img_clahe\n",
    "\n",
    "def simple_dehaze(img, omega=0.95, win_size=15):\n",
    "    norm_img = img.astype(np.float32)/255.0\n",
    "    dark_channel = cv2.erode(np.min(norm_img, axis=2), np.ones((win_size, win_size)))\n",
    "    A = np.max(norm_img, axis=(0,1))\n",
    "    t = 1 - omega * dark_channel[..., np.newaxis]\n",
    "    t = np.clip(t, 0.1, 1)\n",
    "    J = (norm_img - A) / t + A\n",
    "    J = np.clip(J*255, 0, 255).astype(np.uint8)\n",
    "    return J\n",
    "\n",
    "def gray_world_white_balance(img):\n",
    "    img_float = img.astype(np.float32)\n",
    "    avgR = np.mean(img_float[..., 0])\n",
    "    avgG = np.mean(img_float[..., 1])\n",
    "    avgB = np.mean(img_float[..., 2])\n",
    "    avg = (avgR + avgG + avgB) / 3\n",
    "    scaleR, scaleG, scaleB = avg/avgR, avg/avgG, avg/avgB\n",
    "    img_balanced = img_float.copy()\n",
    "    img_balanced[..., 0] *= scaleR\n",
    "    img_balanced[..., 1] *= scaleG\n",
    "    img_balanced[..., 2] *= scaleB\n",
    "    return np.clip(img_balanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "def tensor_to_img(tensor):\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    img = tensor.detach().cpu().permute(1,2,0).numpy()\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "# Perceptual loss using VGG16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = vgg16(pretrained=True).features.eval().to(device)\n",
    "transform_vgg = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def perceptual_loss(img1, img2):\n",
    "    im1 = transform_vgg(img1).unsqueeze(0).to(device)\n",
    "    im2 = transform_vgg(img2).unsqueeze(0).to(device)\n",
    "    f1, f2 = vgg(im1), vgg(im2)\n",
    "    return F.l1_loss(f1, f2).item()\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint_path = Path(\"/kaggle/input/test-1/best_weights_epoch_041.pth\")\n",
    "if not checkpoint_path.is_file():\n",
    "    raise RuntimeError(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "infer_model.to(device)\n",
    "\n",
    "# Use your datamodule to prepare dataloader\n",
    "full_loader = dm.predict_dataloader()\n",
    "\n",
    "results = []\n",
    "\n",
    "metrics_before = {'psnr': [], 'ssim': [], 'perceptual': []}\n",
    "metrics_after = {'psnr': [], 'ssim': [], 'perceptual': []}\n",
    "\n",
    "import math\n",
    "\n",
    "# Run inference and corrections\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(full_loader):\n",
    "        inp, tgt = batch\n",
    "        inp = inp.to(device)\n",
    "        pred = infer_model(inp)\n",
    "        pred_cpu = pred.cpu()\n",
    "\n",
    "        for i in range(inp.size(0)):\n",
    "            orig_img = tensor_to_img(inp[i])\n",
    "            gt_img = tensor_to_img(tgt[i])\n",
    "            pred_img = tensor_to_img(pred_cpu[i])\n",
    "\n",
    "            # Metrics before correction\n",
    "            p_tensor, gt_tensor = pred[i:i+1].to(device), tgt[i:i+1].to(device)\n",
    "            mse = F.mse_loss(p_tensor, gt_tensor)\n",
    "            metrics_before['psnr'].append(10 * torch.log10(1.0 / mse).item())\n",
    "            metrics_before['ssim'].append(ssim_fn(p_tensor, gt_tensor).item())\n",
    "            metrics_before['perceptual'].append(perceptual_loss(pred_img, gt_img))\n",
    "\n",
    "            # Classical corrections\n",
    "            \n",
    "            \n",
    "            clahe_img = clahe_enhancement(pred_img)\n",
    "            #clahe_img = clahe_enhancement(clahe_img)\n",
    "            #dehazed = simple_dehaze(clahe_img)\n",
    "            #gamma_corr_img = gamma_correct(clahe_img, gamma=1.4)\n",
    "            #wb_img = gray_world_white_balance(clahe_img)\n",
    "            #red_boost_img = adaptive_red_boost(clahe_img)\n",
    "            \n",
    "            # Metrics after correction\n",
    "            gamma_tensor = torch.tensor(clahe_img / 255.).unsqueeze(0).permute(0,3,1,2).to(device)\n",
    "            mse_after = F.mse_loss(gamma_tensor, gt_tensor)\n",
    "            metrics_after['psnr'].append(10 * torch.log10(1.0 / mse_after).item())\n",
    "            metrics_after['ssim'].append(ssim_fn(gamma_tensor, gt_tensor).item())\n",
    "            metrics_after['perceptual'].append(perceptual_loss(gamma_corr_img, gt_img))\n",
    "\n",
    "            # Append for visualization\n",
    "            results.append({\n",
    "                'original': orig_img,\n",
    "                'ground_truth': gt_img,\n",
    "                'restored': pred_img,\n",
    "                'dehazed': dehazed,\n",
    "                'clahe_corrected': clahe_img,\n",
    "                'white_balanced': wb_img,\n",
    "                'red_boosted': red_boost_img,\n",
    "                'gamma_corrected': gamma_corr_img\n",
    "            })\n",
    "\n",
    "            if len(results) >= PLOT_LIMIT:\n",
    "                break\n",
    "        if len(results) >= PLOT_LIMIT:\n",
    "            break\n",
    "\n",
    "print(f\"Avg PSNR before corrections: {np.mean(metrics_before['psnr']):.3f} dB\")\n",
    "print(f\"Avg SSIM before corrections: {np.mean(metrics_before['ssim']):.4f}\")\n",
    "print(f\"Avg Perceptual loss before corrections: {np.mean(metrics_before['perceptual']):.4f}\")\n",
    "\n",
    "print(f\"Avg PSNR after corrections: {np.mean(metrics_after['psnr']):.3f} dB\")\n",
    "print(f\"Avg SSIM after corrections: {np.mean(metrics_after['ssim']):.4f}\")\n",
    "print(f\"Avg Perceptual loss after corrections: {np.mean(metrics_after['perceptual']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Postprocessing functions (same as before)\n",
    "def adaptive_red_boost(img):\n",
    "    r_mean = img[..., 0].mean()\n",
    "    g_mean = img[..., 1].mean()\n",
    "    b_mean = img[..., 2].mean()\n",
    "    factor = 2.0 if r_mean < 1 else (g_mean + b_mean) / (2 * r_mean)\n",
    "    factor = np.clip(factor, 1.1, 1.2)\n",
    "    out = img.copy()\n",
    "    out[..., 0] = np.clip(out[..., 0] * factor, 0, 255)\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "def gamma_correct(img, gamma=1.4):\n",
    "    import skimage.exposure as exposure\n",
    "    img_corr = exposure.adjust_gamma(img / 255., gamma)\n",
    "    return (img_corr * 255).astype(np.uint8)\n",
    "\n",
    "def clahe_enhancement(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))\n",
    "    img_clahe = np.empty_like(img)\n",
    "    for i in range(3):\n",
    "        img_clahe[..., i] = clahe.apply(img[..., i])\n",
    "    return img_clahe\n",
    "\n",
    "def simple_dehaze(img, omega=0.95, win_size=15):\n",
    "    norm_img = img.astype(np.float32)/255.0\n",
    "    dark_channel = cv2.erode(np.min(norm_img, axis=2), np.ones((win_size, win_size)))\n",
    "    A = np.max(norm_img, axis=(0,1))\n",
    "    t = 1 - omega * dark_channel[..., np.newaxis]\n",
    "    t = np.clip(t, 0.1, 1)\n",
    "    J = (norm_img - A) / t + A\n",
    "    J = np.clip(J*255, 0, 255).astype(np.uint8)\n",
    "    return J\n",
    "\n",
    "def tensor_to_img(tensor):\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    img = tensor.detach().cpu().permute(1,2,0).numpy()\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = vgg16(pretrained=True).features.eval().to(device)\n",
    "transform_vgg = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def perceptual_loss(img1, img2):\n",
    "    im1 = transform_vgg(img1).unsqueeze(0).to(device)\n",
    "    im2 = transform_vgg(img2).unsqueeze(0).to(device)\n",
    "    f1, f2 = vgg(im1), vgg(im2)\n",
    "    return F.l1_loss(f1, f2).item()\n",
    "\n",
    "# Load model checkpoint and setup model\n",
    "checkpoint_path = Path(\"/kaggle/input/test-1/best_weights_epoch_041.pth\")\n",
    "if not checkpoint_path.is_file():\n",
    "    raise RuntimeError(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "infer_model.to(device)\n",
    "\n",
    "# Loader from your datamodule\n",
    "full_loader = dm.predict_dataloader()\n",
    "\n",
    "# Processing sequences for rows (matching 3 columns total):\n",
    "row1_imgs = ['Camera Input', 'Ground Truth', 'Inference Output']  # original images\n",
    "row2_seqs = [\n",
    "    [clahe_enhancement],\n",
    "    [clahe_enhancement, adaptive_red_boost],\n",
    "    [adaptive_red_boost, clahe_enhancement]\n",
    "]\n",
    "row3_seqs = [\n",
    "    [simple_dehaze, clahe_enhancement, gamma_correct, adaptive_red_boost],\n",
    "    [gamma_correct, simple_dehaze, clahe_enhancement, adaptive_red_boost],\n",
    "    [adaptive_red_boost, clahe_enhancement, simple_dehaze, gamma_correct]\n",
    "]\n",
    "\n",
    "save_dir = Path(\"/kaggle/working/result_images/iter_3\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def apply_sequence(img, seq):\n",
    "    for func in seq:\n",
    "        img = func(img)\n",
    "    return img\n",
    "\n",
    "num_samples = 12\n",
    "indices_to_sample = np.random.choice(len(dm.infer_ds), num_samples, replace=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in indices_to_sample:\n",
    "        inp, tgt = dm.infer_ds[idx]\n",
    "        inp_tensor = inp.unsqueeze(0).to(device)\n",
    "        pred_tensor = infer_model(inp_tensor).cpu()[0]\n",
    "\n",
    "        inp_img = tensor_to_img(inp)\n",
    "        gt_img = tensor_to_img(tgt)\n",
    "        pred_img = tensor_to_img(pred_tensor)\n",
    "\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "        fig.suptitle(f\"Image index: {idx}\", fontsize=16)\n",
    "\n",
    "        # Row 1\n",
    "        row1_images = [inp_img, gt_img, pred_img]\n",
    "        row2_images = [apply_sequence(pred_img, seq) for seq in row2_seqs]\n",
    "        # Row 3\n",
    "        row3_images = [apply_sequence(pred_img, seq) for seq in row3_seqs]\n",
    "        row1_titles = ['Camera Input', 'Ground Truth', 'Inference Output']\n",
    "        row2_titles = [\n",
    "            'CLAHE only', \n",
    "            'CLAHE -> Red Boost', \n",
    "            'Red Boost -> CLAHE'\n",
    "        ]\n",
    "        row3_titles = [\n",
    "            'Dehaze -> CLAHE ->\\nGamma -> Red Boost',\n",
    "            'Gamma -> Dehaze ->\\nCLAHE -> Red Boost',\n",
    "            'Red Boost -> CLAHE ->\\nDehaze -> Gamma'\n",
    "        ]\n",
    "        \n",
    "        # During plotting, use these titles instead of previous ones:\n",
    "        for ax, img, title in zip(axs[0], row1_images, row1_titles):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(title)\n",
    "        \n",
    "        for ax, img, title in zip(axs[1], row2_images, row2_titles):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(title)\n",
    "        \n",
    "        for ax, img, title in zip(axs[2], row3_images, row3_titles):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(title)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        save_path = save_dir / f\"image_{idx}_3x3_processed.png\"\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved 3x3 plot for image {idx} at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT_LIMIT = min(10, len(results))  # max 10 samples or less if fewer available\n",
    "titles = ['Camera Input', 'Ground Truth', 'Restored', 'Dehazed', 'CLAHE Contrast', 'White Balanced', 'Red Boosted', 'Gamma Corrected']\n",
    "keys = [\"original\", \"ground_truth\", \"restored\", \"dehazed\",\n",
    "        \"clahe_corrected\", \"white_balanced\", \"red_boosted\", \"gamma_corrected\"]\n",
    "\n",
    "fig, axs = plt.subplots(PLOT_LIMIT, len(titles), figsize=(3*len(titles), 3*PLOT_LIMIT))\n",
    "\n",
    "for sample_idx in range(PLOT_LIMIT):\n",
    "    for col_idx, (title, key) in enumerate(zip(titles, keys)):\n",
    "        ax = axs[sample_idx, col_idx] if PLOT_LIMIT > 1 else axs[col_idx]\n",
    "        ax.imshow(results[sample_idx][key])\n",
    "        ax.axis(\"off\")\n",
    "        if sample_idx == 0:\n",
    "            ax.set_title(title, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from itertools import combinations, permutations\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Classical post-processing functions\n",
    "def adaptive_red_boost(img):\n",
    "    r_mean = img[..., 0].mean()\n",
    "    g_mean = img[..., 1].mean()\n",
    "    b_mean = img[..., 2].mean()\n",
    "    factor = 2.0 if r_mean < 1 else (g_mean + b_mean) / (2 * r_mean)\n",
    "    factor = np.clip(factor, 1.1, 1.2)\n",
    "    out = img.copy()\n",
    "    out[..., 0] = np.clip(out[..., 0] * factor, 0, 255)\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "def gamma_correct(img, gamma=1.4):\n",
    "    import skimage.exposure as exposure\n",
    "    img_corr = exposure.adjust_gamma(img / 255., gamma)\n",
    "    return (img_corr * 255).astype(np.uint8)\n",
    "\n",
    "def clahe_enhancement(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))\n",
    "    img_clahe = np.empty_like(img)\n",
    "    for i in range(3):\n",
    "        img_clahe[..., i] = clahe.apply(img[..., i])\n",
    "    return img_clahe\n",
    "\n",
    "def simple_dehaze(img, omega=0.95, win_size=15):\n",
    "    norm_img = img.astype(np.float32)/255.0\n",
    "    dark_channel = cv2.erode(np.min(norm_img, axis=2), np.ones((win_size, win_size)))\n",
    "    A = np.max(norm_img, axis=(0,1))\n",
    "    t = 1 - omega * dark_channel[..., np.newaxis]\n",
    "    t = np.clip(t, 0.1, 1)\n",
    "    J = (norm_img - A) / t + A\n",
    "    J = np.clip(J*255, 0, 255).astype(np.uint8)\n",
    "    return J\n",
    "\n",
    "def tensor_to_img(tensor):\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    img = tensor.detach().cpu().permute(1,2,0).numpy()\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "# Perceptual loss model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = vgg16(pretrained=True).features.eval().to(device)\n",
    "transform_vgg = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def perceptual_loss(img1, img2):\n",
    "    im1 = transform_vgg(img1).unsqueeze(0).to(device)\n",
    "    im2 = transform_vgg(img2).unsqueeze(0).to(device)\n",
    "    f1, f2 = vgg(im1), vgg(im2)\n",
    "    return F.l1_loss(f1, f2).item()\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint_path = Path(\"/kaggle/input/test-1/best_weights_epoch_041.pth\")\n",
    "if not checkpoint_path.is_file():\n",
    "    raise RuntimeError(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "\n",
    "infer_model = ResNet34_UNet_CBAM(pretrained=False)\n",
    "state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "infer_model.load_state_dict(state)\n",
    "infer_model.eval()\n",
    "infer_model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Use datamodule's dataset for prediction\n",
    "full_dataset = dm.infer_ds\n",
    "\n",
    "\n",
    "# Sample a random subset for faster processing\n",
    "num_samples = 200\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(full_dataset), num_samples, replace=False)\n",
    "subset_dataset = Subset(full_dataset, sample_indices)\n",
    "\n",
    "# New DataLoader for subset\n",
    "subset_loader = torch.utils.data.DataLoader(subset_dataset,\n",
    "                                            batch_size=dm.predict_dataloader().batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=dm.predict_dataloader().num_workers,\n",
    "                                            pin_memory=True)\n",
    "\n",
    "# Map names to functions\n",
    "pp_funcs = {\n",
    "    'clahe': clahe_enhancement,\n",
    "    'red_boost': adaptive_red_boost,\n",
    "    'gamma': gamma_correct,\n",
    "    'dehaze': simple_dehaze\n",
    "}\n",
    "\n",
    "proc_names = list(pp_funcs.keys())\n",
    "proc_orders = [()]\n",
    "for r in range(1, len(proc_names)+1):\n",
    "    for comb in combinations(proc_names, r):\n",
    "        for perm in permutations(comb):\n",
    "            proc_orders.append(perm)\n",
    "print(f\"Total permutations: {len(proc_orders)}\")\n",
    "\n",
    "metrics_summary = {seq: {'psnr': [], 'ssim': [], 'perceptual': []} for seq in proc_orders}\n",
    "\n",
    "def apply_processing(img, seq):\n",
    "    out = img\n",
    "    for p in seq:\n",
    "        out = pp_funcs[p](out)\n",
    "    return out\n",
    "\n",
    "with torch.no_grad():\n",
    "    for perm_idx, seq in enumerate(proc_orders):\n",
    "        print(f\"\\nProcessing permutation {perm_idx+1}/{len(proc_orders)}: {' -> '.join(seq) if seq else 'no_processing'}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(subset_loader):\n",
    "            inp, tgt = batch\n",
    "            inp = inp.to(device)\n",
    "            preds = infer_model(inp).cpu()\n",
    "\n",
    "            for i in range(inp.shape[0]):\n",
    "                pred_img = tensor_to_img(preds[i])\n",
    "                gt_img = tensor_to_img(tgt[i])\n",
    "\n",
    "                processed_img = apply_processing(pred_img, seq)\n",
    "\n",
    "                proc_tensor = torch.tensor(processed_img/255.).permute(2,0,1).unsqueeze(0).to(device)\n",
    "                gt_tensor = torch.tensor(gt_img/255.).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "                mse = F.mse_loss(proc_tensor, gt_tensor)\n",
    "                psnr = 10 * torch.log10(1.0 / mse).item()\n",
    "                ssim_val = ssim_fn(proc_tensor, gt_tensor).item()\n",
    "                perc_loss = perceptual_loss(processed_img, gt_img)\n",
    "\n",
    "                metrics_summary[seq]['psnr'].append(psnr)\n",
    "                metrics_summary[seq]['ssim'].append(ssim_val)\n",
    "                metrics_summary[seq]['perceptual'].append(perc_loss)\n",
    "\n",
    "out_file = \"/kaggle/working/post_processing_permutations_metrics.txt\"\n",
    "with open(out_file, 'w') as f:\n",
    "    for seq, metrics in metrics_summary.items():\n",
    "        psnr_avg = np.mean(metrics['psnr']) if metrics['psnr'] else float('nan')\n",
    "        ssim_avg = np.mean(metrics['ssim']) if metrics['ssim'] else float('nan')\n",
    "        perc_avg = np.mean(metrics['perceptual']) if metrics['perceptual'] else float('nan')\n",
    "\n",
    "        line = f\"Sequence: {' -> '.join(seq) if seq else 'no_processing'}\\n\"\n",
    "        line += f\"  Avg PSNR  : {psnr_avg:.4f} dB\\n\"\n",
    "        line += f\"  Avg SSIM  : {ssim_avg:.4f}\\n\"\n",
    "        line += f\"  Avg Perceptual Loss : {perc_avg:.4f}\\n\"\n",
    "        line += \"-\"*50 + \"\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "print(f\"All done! Metrics saved to {out_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8545778,
     "sourceId": 13481855,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8692798,
     "sourceId": 13671273,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
